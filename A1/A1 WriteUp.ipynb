{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given by the assignment\n",
    "def loadData():\n",
    "    with np.load('notMNIST.npz') as data :\n",
    "        Data, Target = data['images'], data['labels']\n",
    "        posClass = 2\n",
    "        negClass = 9\n",
    "        dataIndx = (Target==posClass) + (Target==negClass)\n",
    "        Data = Data[dataIndx]/255.\n",
    "        Target = Target[dataIndx].reshape(-1, 1)\n",
    "        Target[Target==posClass] = 1\n",
    "        Target[Target==negClass] = 0\n",
    "        np.random.seed(421)\n",
    "        randIndx = np.arange(len(Data))\n",
    "        np.random.shuffle(randIndx)\n",
    "        Data, Target = Data[randIndx], Target[randIndx]\n",
    "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "        testData, testTarget = Data[3600:], Target[3600:]\n",
    "    return trainData, validData, testData, trainTarget, validTarget, testTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Visualizing Dataset\n",
    "\n",
    "It's always important to visualize the dataset to gain and understanding of what the model is trying to accomplish. This can help in the debugging phase. The shape of each data set is printed out as well as a random sample of the training data is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "print(f\"Training Data: {trainData.shape}\\tTraining tagets: {trainTarget.shape}\")\n",
    "print(f\"Validation Data: {validData.shape}\\tValidation tagets: {validTarget.shape}\")\n",
    "print(f\"Testing Data: {testData.shape}\\tTesting tagets:{testTarget.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(image, target, ax=None):\n",
    "    ax = plt.gca() if ax == None else ax\n",
    "    ax.imshow(image, cmap=\"hot\")\n",
    "    ax.set_title('J' if target == 0 else 'C')\n",
    "    # targets are binary encoded 0 == 'J' and 1 == 'C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axis = plt.subplots(2, 5, figsize=(16, 5))\n",
    "for ax in axis.reshape(-1):\n",
    "    r = np.random.randint(trainData.shape[0])\n",
    "    plot(trainData[r], trainTarget[r], ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful functions that will be used throughout the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(X, w, b):\n",
    "    # flatten X\n",
    "    if len(X.shape) == 3:\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "    \n",
    "    # insert 1's at position 0 along the columns\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    # insert b at the front of W\n",
    "    w = np.insert(w, 0, b, axis=0)\n",
    "    \n",
    "    return X, w\n",
    "\n",
    "def get_zero_parameters():\n",
    "    w = np.zeros(d)\n",
    "    b = np.zeros(1)\n",
    "    return w, b\n",
    "\n",
    "def get_random_parameters():\n",
    "    w = np.random.uniform(low=-1.0, high=1.0, size=(d,))\n",
    "    b = np.random.uniform(low=-1.0, high=1.0, size=(1,))\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    return X.dot(w) + b\n",
    "\n",
    "def accuracy(w, b, X, y):\n",
    "    y = y.reshape(-1)\n",
    "    y_pred = predict(w, b, X)\n",
    "    y_pred = np.vectorize(lambda z: 1 if z > 0 else 0)(y_pred)\n",
    "    return sum(y_pred == y) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(x, train_loss=None, valid_loss=None, test_loss=None, title=None, ax=None):\n",
    "    ax = plt.gca() if ax == None else ax\n",
    "    if train_loss != None:\n",
    "        ax.plot(x, train_loss, label=\"Training Loss\")\n",
    "    if valid_loss != None:\n",
    "        ax.plot(x, valid_loss, label=\"Validation Loss\")\n",
    "    if test_loss != None:\n",
    "        ax.plot(x, test_loss, label=\"Testing Loss\")\n",
    "    \n",
    "    ax.set_title(\"Loss\" if title == None else title)\n",
    "    \n",
    "    ax.set_xlabel(\"Iterations\")\n",
    "    ax.set_xlim(left=0)\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "def plot_accuracy(x, train_accuracy=None, valid_accuracy=None, test_accuracy=None, title=None, ax=None):\n",
    "    ax = plt.gca() if ax == None else ax\n",
    "    if train_accuracy != None:\n",
    "        ax.plot(x, train_accuracy, label=\"Training Accuracy\")\n",
    "    if valid_accuracy != None:\n",
    "        ax.plot(x, valid_accuracy, label=\"Validation Accuracy\")\n",
    "    if test_accuracy != None:\n",
    "        ax.plot(x, test_accuracy, label=\"Testing Accuracy\")\n",
    "    \n",
    "    ax.set_title(\"Accuracy\" if title == None else title)\n",
    "\n",
    "    ax.set_xlabel(\"Iterations\")\n",
    "    ax.set_xlim(left=0)\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_yticks(np.arange(0, 1.1, step=0.1))\n",
    "    ax.grid(linestyle='-', axis='y')\n",
    "    ax.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some global variables that will be used throughout the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "VTDatasets = {\"validData\" : validData, \"validTarget\" : validTarget,\n",
    "              \"testData\" : testData, \"testTarget\" : testTarget}\n",
    "\n",
    "N = trainData.shape[0]\n",
    "d = trainData.shape[1] * trainData.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Linear Regression\n",
    "### 1. Loss Function and Gradient\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(n)} = W^T \\textbf{x}^{(n)} + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{MSE} = \\frac{1}{N}\\sum_{n=1}^N(\\hat{y}^{(n)} - y^{(n)})^2 + \\lambda \\Vert W \\Vert_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{MSE}}{\\partial b} = \\frac{2}{N} \\sum_{n=1}^N (\\hat{y}^{(n)} - y^{(n)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{MSE}}{\\partial W} = \\frac{2}{N} \\ X^T \\left ( \\hat{\n",
    "\\textbf{y}} - \\textbf{y} \\right ) + \\lambda W\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(w, b, X, y, reg):\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    y = y.reshape(-1)\n",
    "    return np.square(X.dot(w) + b - y).mean() + reg * np.square(w).sum()\n",
    "\n",
    "def gradMSE(w, b, X, y, reg):\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    y = y.reshape(-1)\n",
    "    N = y.shape[0]\n",
    "    \n",
    "    w_grad = 2.0/N * X.T.dot(X.dot(w) + b - y) + reg * w\n",
    "    b_grad = 2.0/N * np.sum(X.dot(w) + b - y)\n",
    "    return w_grad, b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent_MSE(w, b, X, y, alpha, epochs, reg, error_tol=1e-7, \n",
    "                     validData=None, validTarget=None, testData=None, testTarget=None):\n",
    "    train_loss, train_acc = [], []\n",
    "    valid_loss, valid_acc = [], []\n",
    "    test_loss, test_acc = [], []\n",
    "    for i in range(epochs):\n",
    "        grad_w, grad_b = gradMSE(w, b, X, y, reg)\n",
    "        w -= alpha * grad_w\n",
    "        b -= alpha * grad_b\n",
    "        \n",
    "        # Calculating Statistics\n",
    "        train_loss.append( MSE(w, b, X, y, reg) )\n",
    "        train_acc.append( accuracy(w, b, X, y) )\n",
    "\n",
    "        if not validData is None and not validTarget is None:\n",
    "            valid_loss.append( MSE(w, b, validData, validTarget, reg) )\n",
    "            valid_acc.append( accuracy(w, b, validData, validTarget) )\n",
    "        if not testData is None and not testTarget is None:\n",
    "            test_loss.append( MSE(w, b, testData, testTarget, reg) )\n",
    "            test_acc.append( accuracy(w, b, testData, testTarget) )\n",
    "\n",
    "        # Check stopping condition\n",
    "        if i > 1 and np.abs(train_loss[-2] - train_loss[-1]) <= error_tol:\n",
    "            break\n",
    "\n",
    "    statistics = (train_loss, train_acc)\n",
    "    if not validData is None and not validTarget is None:\n",
    "        statistics += (valid_loss, valid_acc, )\n",
    "    if not testData is None and not testTarget is None:\n",
    "        statistics += (test_loss, test_acc,)\n",
    "    # Python 3.8 made this easier, but 3.7 you have to do this\n",
    "    out = (w, b, *statistics)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tuning the Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation of Gradient Descent with 5000 epochs and \\lambda = 0. Investigate the\n",
    "# impact of learning rate, \\alpha = 0.005, 0.001, 0.0001 on the performance of your classifier. \n",
    "# Plot the training, validation and test losses.\n",
    "\n",
    "for alpha in [0.005, 0.001, 0.0001]:\n",
    "    \n",
    "    print(\"alpha =\", alpha)\n",
    "    \n",
    "    w, b = get_random_parameters()\n",
    "    w, b, *statistics = grad_descent_MSE(w, b, trainData, trainTarget, \n",
    "                                         alpha=alpha, \n",
    "                                         epochs=5000, \n",
    "                                         reg=0, \n",
    "                                         **VTDatasets)\n",
    "    train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "    \n",
    "    print(f\"Training loss: {train_loss[-1]:.4f}{'':20s}Training acc: {train_acc[-1]*100:.2f}%\")\n",
    "    print(f\"Validation loss: {valid_loss[-1]:.4f}{'':18s}Validation acc: {valid_acc[-1]*100:.2f}%\")\n",
    "    print(f\"Testing loss: {test_loss[-1]:.4f}{'':21s}Testing acc: {test_acc[-1]*100:.2f}%\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "    plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate impact by modifying the regularization parameter, \\lambda = {0.001, 0.1, 0.5}. \n",
    "# Plot the training, validation and test loss for \\alpha = 0:005 and report the final training, \n",
    "# validation and test accuracy of your classifier.\n",
    "\n",
    "for reg in [0.001, 0.1, 0.5]:\n",
    "    \n",
    "    print(\"regularization =\", reg)\n",
    "    \n",
    "    w, b = get_random_parameters()\n",
    "    w, b, *statistics = grad_descent_MSE(w, b, trainData, trainTarget, \n",
    "                                         alpha=0.005, \n",
    "                                         epochs=5000, \n",
    "                                         reg=reg, \n",
    "                                         **VTDatasets)\n",
    "    train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "    \n",
    "    print(f\"Training loss: {train_loss[-1]:.4f}{'':.20s}\\tTraining acc: {train_acc[-1]*100:.2f}%\")\n",
    "    print(f\"Validation loss: {valid_loss[-1]:.4f}\\tValidation acc: {valid_acc[-1]*100:.2f}%\")\n",
    "    print(f\"Testing loss: {test_loss[-1]:.4f}\\tTesting acc: {test_acc[-1]*100:.2f}%\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "    plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Comparing Batch GD with normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(X, y):\n",
    "    N = X.shape[0]\n",
    "    d = X.shape[1] * X.shape[2]\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    y = y.reshape(-1)\n",
    "    \n",
    "    # overparameterized (deep learning)\n",
    "    if N < d:\n",
    "        w_aug = X.T @ np.linalg.inv( X @ X.T ) @ y\n",
    "    \n",
    "    # underparameterized (typical case)\n",
    "    else:\n",
    "        w_aug = np.linalg.inv( X.T @ X ) @ X.T @ y\n",
    "    \n",
    "    return w_aug[1:], w_aug[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Squares Training loss: 0.0187\tLeast Squares Training acc: 71.29%\n",
      "Least Squares Validation loss: 0.0476\tLeast Squares Validation acc: 69.00%\n",
      "Least Squares Testing loss: 0.0570\tLeast Squares Testing acc: 66.90%\n"
     ]
    }
   ],
   "source": [
    "# compare above to gradient descent solution\n",
    "w_LS, b_LS = least_squares(trainData, trainTarget)\n",
    "\n",
    "loss = MSE(w_LS, b_LS, trainData, trainTarget, 0)\n",
    "acc = accuracy(w_LS, b_LS, trainData, trainTarget)\n",
    "print(f\"Least Squares Training loss: {loss:.4f}\\tLeast Squares Training acc: {100*acc:.2f}%\")\n",
    "loss = MSE(w_LS, b_LS, validData, validTarget, 0)\n",
    "acc = accuracy(w_LS, b_LS, validData, validTarget)\n",
    "print(f\"Least Squares Validation loss: {loss:.4f}\\tLeast Squares Validation acc: {100*acc:.2f}%\")\n",
    "loss = MSE(w_LS, b_LS, testData, testTarget, 0)\n",
    "acc = accuracy(w_LS, b_LS, testData, testTarget)\n",
    "print(f\"Least Squares Testing loss: {loss:.4f}\\tLeast Squares Testing acc: {100*acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the analytical solution, the training loss achieved with the analytical equation is 0.0187 with a training accuracy of 71.29%. The training loss and accuracies for batch gradient descent were respectively 0.6918 and 67.97%. From the values, we see that the analytical solution performed better. However, computing it grows increasingly difficult with the size of the problem. As the problem scales, batch gradient descent allows for faster convergence with comparable accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Logistic Regression\n",
    "## 2.1 Binary cross-entropy loss\n",
    "### 1. Loss Function and Gradient\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(n)} = \\sigma(W^T\\textbf{x}^{(n)} + b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{CE} = \\frac{1}{N}\\sum_{n=1}^{N} \\left [ -y^{(n)} \\log(\\hat{y}^{(n)}) -(1- y^{(n)}) \\log (1 - \\hat{y}^{(n)} ) \\right ] + \\frac{\\lambda}{2} \\Vert W \\Vert^2_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{CE}}{\\partial b} = \\frac{1}{N} \\sum_{n=1}^{N} \\left [ \\hat{y}^{(n)} - y^{(n)} \\right ]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{CE}}{\\partial W} = \\frac{1}{N} \\sum_{n=1}^{N} \\left [ (\\hat{y}^{(n)} - y^{(n)}) \\ \\textbf{x}^{(n)} \\right ] + \\lambda W\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "    \n",
    "def crossEntropyLoss(w, b, X, y, reg):    \n",
    "    X, w = augment(X, w, b)\n",
    "    y = y.reshape(-1)\n",
    "    N = y.shape[0]\n",
    "    \n",
    "    y_hat = sigmoid(X.dot(w))\n",
    "    \n",
    "    return 1.0/N * (-y.dot(np.log(y_hat+1e-20)) - (1 - y).dot(np.log(1 - y_hat+1e-20))) + reg/2.0 * np.square(w[1:]).sum()\n",
    "    \n",
    "def gradCE(w, b, X, y, reg):\n",
    "    X, w = augment(X, w, b)\n",
    "    y = y.reshape(-1)\n",
    "    N = y.shape[0]\n",
    "    \n",
    "    y_hat = sigmoid(X.dot(w))\n",
    "    \n",
    "    grad = 1.0 /N * X.T.dot(y_hat - y) + reg * w\n",
    "    \n",
    "    return grad[1:], grad[0] - reg * w[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(w, b, X, y, alpha, epochs, reg, error_tol=1e-7, lossType=\"MSE\", \n",
    "                 validData=None, validTarget=None, testData=None, testTarget=None):\n",
    "    loss_func, grad_func = None, None\n",
    "    if lossType == \"MSE\":\n",
    "        loss_func, grad_func = MSE, gradMSE\n",
    "    elif lossType == \"CE\":\n",
    "        loss_func, grad_func = crossEntropyLoss, gradCE\n",
    "    else:\n",
    "        raise ValueError(\"Variable 'lossType' must be either 'MSE' or 'CE'.\")\n",
    "    \n",
    "    train_loss, train_acc = [], []\n",
    "    valid_loss, valid_acc = [], []\n",
    "    test_loss, test_acc = [], []\n",
    "    printing = False\n",
    "    for i in range(epochs):\n",
    "        grad_w, grad_b = grad_func(w, b, X, y, reg)\n",
    "        w -= alpha * grad_w\n",
    "        b -= alpha * grad_b\n",
    "\n",
    "        # Calculating Statistics\n",
    "        train_loss.append(loss_func(w, b, X, y, reg))\n",
    "        train_acc.append(accuracy(w, b, X, y))\n",
    "\n",
    "        if not validData is None and not validTarget is None:\n",
    "            valid_loss.append(loss_func(w, b, validData, validTarget, reg))\n",
    "            valid_acc.append(accuracy(w, b, validData, validTarget))\n",
    "        if not testData is None and not testTarget is None:\n",
    "            test_loss.append(loss_func(w, b, testData, testTarget, reg))\n",
    "            test_acc.append(accuracy(w, b, testData, testTarget))\n",
    "\n",
    "        # Check stopping condition\n",
    "        if i > 1 and np.abs(train_loss[-2] - train_loss[-1]) <= error_tol:\n",
    "            break\n",
    "\n",
    "    statistics = (train_loss, train_acc)\n",
    "    if not validData is None and not validTarget is None:\n",
    "        statistics += (valid_loss, valid_acc,)\n",
    "    if not testData is None and not testTarget is None:\n",
    "        statistics += (test_loss, test_acc,)\n",
    "    out = (w, b, *statistics)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = get_random_parameters()\n",
    "w, b, *statistics = grad_descent(w, b, trainData, trainTarget, \n",
    "                                 alpha=0.005, \n",
    "                                 epochs=5000, \n",
    "                                 reg=0.1,\n",
    "                                 lossType='CE',\n",
    "                                 **VTDatasets)\n",
    "train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "\n",
    "print(f\"Training loss: {train_loss[-1]:.4f}{'':.20s}\\tTraining acc: {train_acc[-1]*100:.2f}%\")\n",
    "print(f\"Validation loss: {valid_loss[-1]:.4f}\\tValidation acc: {valid_acc[-1]*100:.2f}%\")\n",
    "print(f\"Testing loss: {test_loss[-1]:.4f}\\tTesting acc: {test_acc[-1]*100:.2f}%\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Comparision to Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For zero weight decay, learning rate of 0.005 and 5000 epochs, \n",
    "# plot the training cross entropy loss and MSE loss for \n",
    "# logistic regression and linear regression respectively.\n",
    "# Comment on the effect of cross-entropy loss convergence behaviour.\n",
    "\n",
    "w, b = get_random_parameters()\n",
    "\n",
    "# Linear Regresesion\n",
    "print(\"Linear Regression\")\n",
    "w_lin, b_lin, *statistics = grad_descent(w, b, trainData, trainTarget, \n",
    "                                         alpha=0.005, \n",
    "                                         epochs=5000, \n",
    "                                         reg=0,\n",
    "                                         lossType='MSE',\n",
    "                                         **VTDatasets)\n",
    "train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "\n",
    "print(f\"Training loss: {train_loss[-1]:.4f}{'':.20s}\\tTraining acc: {train_acc[-1]*100:.2f}%\")\n",
    "print(f\"Validation loss: {valid_loss[-1]:.4f}\\tValidation acc: {valid_acc[-1]*100:.2f}%\")\n",
    "print(f\"Testing loss: {test_loss[-1]:.4f}\\tTesting acc: {test_acc[-1]*100:.2f}%\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"Logistic Regression\")\n",
    "w_log, b_log, *statistics = grad_descent(w, b, trainData, trainTarget, \n",
    "                                         alpha=0.005, \n",
    "                                         epochs=5000, \n",
    "                                         reg=0,\n",
    "                                         lossType='CE',\n",
    "                                         **VTDatasets)\n",
    "train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "\n",
    "print(f\"Training loss: {train_loss[-1]:.4f}{'':.20s}\\tTraining acc: {train_acc[-1]*100:.2f}%\")\n",
    "print(f\"Validation loss: {valid_loss[-1]:.4f}\\tValidation acc: {valid_acc[-1]*100:.2f}%\")\n",
    "print(f\"Testing loss: {test_loss[-1]:.4f}\\tTesting acc: {test_acc[-1]*100:.2f}%\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "plt.show()\n",
    "\n",
    "# Sandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Batch Gradient Descent vs. SGD and Adam\n",
    "## 3.1 SGD\n",
    "### 1. Building the Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildGraph(beta1=None, beta2=None, epsilon=None, alpha=0.001, loss=\"MSE\", seed=421, regulizer=5):\n",
    "    # Initialize weight and bias tensors\n",
    "    tf.set_random_seed(seed)\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    \n",
    "\n",
    "    # Getting small random initial values for weights and bias\n",
    "    w = tf.truncated_normal([d, 1], stddev=0.5)\n",
    "    b = tf.truncated_normal([1, 1], stddev=0.5)\n",
    "\n",
    "    # Converting into tensorflow Variable objects\n",
    "    w = tf.Variable(w, name=\"weights\")\n",
    "    b = tf.Variable(b, name=\"bias\")\n",
    "\n",
    "    # tensorflow objects for data\n",
    "    X = tf.placeholder(tf.float32, (None, d))\n",
    "    y = tf.placeholder(tf.float32, (None, 1))\n",
    "    reg = regulizer\n",
    "\n",
    "    y_pred = None\n",
    "    loss_tensor = None\n",
    "    if loss == \"MSE\":\n",
    "        y_pred = tf.matmul(X, w) + b\n",
    "        loss_tensor = tf.losses.mean_squared_error(y, y_pred) + reg * tf.matmul(w, t)\n",
    "    elif loss == \"CE\":\n",
    "        y_pred = tf.sigmoid(tf.matmul(X, w) + b)\n",
    "        loss_tensor = tf.losses.sigmoid_cross_entropy(y, y_pred)\n",
    "    else:\n",
    "        raise ValueError(\"Variable 'lossType' must be either 'MSE' or 'CE'.\")\n",
    "\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=alpha, beta1=beta1, beta2=beta2, epsilon=epsilon)\n",
    "    opt_op = opt.minimize(loss_tensor)\n",
    "\n",
    "    return w, b, reg, X, y, y_pred, loss_tensor, opt_op\n",
    "\n",
    "# test call to see output\n",
    "# buildGraph(trainData, trainTarget, beta1=0.95, beta2=0.99, epsilon=10^-5, alpha=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementing Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005 cost= 5.754683495\n",
      "Epoch: 0010 cost= 3.821732521\n",
      "Epoch: 0015 cost= 2.511814356\n",
      "Epoch: 0020 cost= 1.893908262\n",
      "Epoch: 0025 cost= 1.444497228\n",
      "Epoch: 0030 cost= 1.120173812\n",
      "Epoch: 0035 cost= 0.887835324\n",
      "Epoch: 0040 cost= 0.718894780\n",
      "Epoch: 0045 cost= 0.595800817\n",
      "Epoch: 0050 cost= 0.503281236\n",
      "Epoch: 0055 cost= 0.424858510\n",
      "Epoch: 0060 cost= 0.375856817\n",
      "Epoch: 0065 cost= 0.319231451\n",
      "Epoch: 0070 cost= 0.273997068\n",
      "Epoch: 0075 cost= 0.238362044\n",
      "Epoch: 0080 cost= 0.208502844\n",
      "Epoch: 0085 cost= 0.184109628\n",
      "Epoch: 0090 cost= 0.165118515\n",
      "Epoch: 0095 cost= 0.151694283\n",
      "Epoch: 0100 cost= 0.136432514\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "display_step = 5\n",
    "\n",
    "w, b, reg, data, labels, pred_labels, loss_tensor, opt_op = buildGraph(beta1=0.95, beta2=0.99, epsilon=1e-4, alpha=0.01, loss=\"MSE\")\n",
    "X = trainData.reshape(trainData.shape[0], -1)\n",
    "Y = trainTarget\n",
    "batch_iter = BatchLoader((X, Y), batch_size=500, randomize=False)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in batch_iter:\n",
    "            sess.run(opt_op, feed_dict={data: x, labels: y})\n",
    "\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            c = sess.run(loss_tensor, feed_dict={data: x, labels: y})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader(object):\n",
    "\n",
    "    def __init__(self, data, batch_size=None, randomize=True, drop_last=False, seed=None):\n",
    "    \n",
    "        # error checking\n",
    "        if len(data) > 1:\n",
    "            for i in range(len(data)-1):\n",
    "                if data[i].shape[0] != data[i+1].shape[0]:\n",
    "                    raise ValueError(\"All inputs must have the same number of elements\")\n",
    "    \n",
    "        self.data = data if type(data) == tuple else (data, )\n",
    "        self.N = data[0].shape[0]\n",
    "        self.batch_size = batch_size if batch_size != None else self.N\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        # shuffling data\n",
    "        if randomize:\n",
    "            indices = np.arange(self.N)\n",
    "            np.random.seed(seed)\n",
    "            np.random.shuffle(indices)\n",
    "            self.data = tuple([d[indices] for d in self.data])\n",
    "    \n",
    "        self.index = 0 \n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "    \n",
    "        # stop condition\n",
    "        if self.index >= self.N:\n",
    "            self.index = 0          # resetting index for next iteration\n",
    "            raise StopIteration\n",
    "\n",
    "        # iterating\n",
    "        self.index += self.batch_size\n",
    "    \n",
    "        if self.index > self.N:\n",
    "            if self.drop_last:\n",
    "                self.index = 0      # resetting index for next iteration\n",
    "                raise StopIteration\n",
    "            else:\n",
    "                return tuple([ d[self.index - self.batch_size: ] for d in self.data ])\n",
    "        else:\n",
    "            return tuple([ d[self.index - self.batch_size: self.index] for d in self.data ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(X, y, alpha, epochs, reg, error_tol=1e-7, lossType=\"CE\",\n",
    "              batch_size=500, randomize=True, beta1=None, beta2=None, epsilon=None,\n",
    "              validData=None, validTarget=None, testData=None, testTarget=None):\n",
    "    \n",
    "    optimizer, *_ = buildGraph(beta1=beta1, beta2=beta2, epsilon=epsilon, \n",
    "                               alpha=alpha, loss=lossType)\n",
    "    \n",
    "    train_loss, train_acc = [], []\n",
    "    valid_loss, valid_acc = [], []\n",
    "    test_loss, test_acc = [], []\n",
    "    printing = False\n",
    "    \n",
    "    batch_iter = BatchLoader((X, y), batch_size=batch_size, randomize=randomize)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        for batch, targets in batch_iter:\n",
    "            \n",
    "            # do the update using the optimizer\n",
    "            \n",
    "            # Calculating Statistics\n",
    "            running_loss += loss_func(w, b, batch, targets, reg) * batch.shape[0]\n",
    "            running_acc += accuracy(w, b, batch, targets) * batch.shape[0]\n",
    "            \n",
    "            # Check stopping condition\n",
    "            if i > 1 and np.abs(train_loss[-2] - train_loss[-1]) <= error_tol:\n",
    "                break\n",
    "        else:\n",
    "            \"\"\"All of this will probably need to change slightly\"\"\"\n",
    "            # Calculating Statistics\n",
    "            train_loss.append(running_loss / X.shape[0])\n",
    "            train_acc.append(running_acc / X.shape[0])\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            \n",
    "            if not validData is None and not validTarget is None:\n",
    "                valid_loss.append(loss_func(w, b, validData, validTarget, reg))\n",
    "                valid_acc.append(accuracy(w, b, validData, validTarget))\n",
    "            if not testData is None and not testTarget is None:\n",
    "                test_loss.append(loss_func(w, b, testData, testTarget, reg))\n",
    "                test_acc.append(accuracy(w, b, testData, testTarget))\n",
    "            \n",
    "            continue\n",
    "        break\n",
    "\n",
    "    statistics = (train_loss, train_acc)\n",
    "    if not validData is None and not validTarget is None:\n",
    "        statistics += (valid_loss, valid_acc,)\n",
    "    if not testData is None and not testTarget is None:\n",
    "        statistics += (test_loss, test_acc,)\n",
    "    out = (w, b, *statistics)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the SGD algorithm for a minibatch size of 500\n",
    "# optimizing over 700 epochs, minimizing the MSE (you will repeat this for the CE later).\n",
    "# Calculate the total number of batches required by dividing the number\n",
    "# of training instances by the minibatch size. After each epoch you will need to reshuffle the\n",
    "# training data and start sampling from the beginning again. Initially, set \\lambda = 0 and continue\n",
    "# to use the same \\alpha value (i.e. 0.001). After each epoch, store the training, validation and test\n",
    "# losses and accuracies. Use these to plot the loss and accuracy curves.\n",
    "\n",
    "w, b = get_random_parameters()\n",
    "w, b, *statistics = SGD(trainData, trainTarget, \n",
    "                        alpha=0.001, \n",
    "                        epochs=700, \n",
    "                        reg=0,\n",
    "                        lossType='MSE',\n",
    "                        batch_size=500,\n",
    "                        **VTDatasets)\n",
    "train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Batch Size Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size in [100, 700, 1750]:\n",
    "    \n",
    "    print(\"batch size =\", batch_size)\n",
    "    \n",
    "    w, b = get_random_parameters()\n",
    "    w, b, *statistics = SGD(w, b, trainData, trainTarget, \n",
    "                            alpha=0.001, \n",
    "                            epochs=700, \n",
    "                            reg=0,\n",
    "                            lossType='CE',\n",
    "                            batch_size=500,\n",
    "                            **VTDatasets)\n",
    "    train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "    \n",
    "    print(f\"Training loss: {train_loss[-1]:.4f}{'':20s}Training acc: {train_acc[-1]*100:.2f}%\")\n",
    "    print(f\"Validation loss: {valid_loss[-1]:.4f}{'':18s}Validation acc: {valid_acc[-1]*100:.2f}%\")\n",
    "    print(f\"Testing loss: {test_loss[-1]:.4f}{'':21s}Testing acc: {test_acc[-1]*100:.2f}%\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "    plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hyperparameter Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for beta1 in [0.95, 0.99]:\n",
    "    for beta2 in [0.99, 0.9999]:\n",
    "        for epsilon in [1e-9, 1e-4]:\n",
    "            \n",
    "            print(f\"beta1: {beta1}\\tbeta2: {beta2}\\tepsilon: {epsilon}\")\n",
    "            \n",
    "            \"\"\" Call SGD\n",
    "            w, b, *statistics = SGD(w, b, trainData, trainTarget, \n",
    "                            alpha=0.001, \n",
    "                            epochs=700, \n",
    "                            reg=0,\n",
    "                            lossType='MSE',\n",
    "                            batch_size=500,\n",
    "                            **VTDatasets)\n",
    "            \"\"\"\n",
    "            train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "    \n",
    "            print(f\"Training loss: {train_loss[-1]:.4f}{'':20s}Training acc: {train_acc[-1]*100:.2f}%\")\n",
    "            print(f\"Validation loss: {valid_loss[-1]:.4f}{'':18s}Validation acc: {valid_acc[-1]*100:.2f}%\")\n",
    "            print(f\"Testing loss: {test_loss[-1]:.4f}{'':21s}Testing acc: {test_acc[-1]*100:.2f}%\")\n",
    "\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "            plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "            plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Cross Entropy Loss Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1.2 with Cross Entropy Loss\n",
    "w, b = get_random_parameters()\n",
    "w, b, *statistics = SGD(w, b, trainData, trainTarget, \n",
    "                        alpha=0.001, \n",
    "                        epochs=700, \n",
    "                        reg=0,\n",
    "                        lossType='CE',\n",
    "                        batch_size=500,\n",
    "                        **VTDatasets)\n",
    "train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "plt.show()\n",
    "\n",
    "# 3.1.4 with Cross Entropy Loss\n",
    "for beta1 in [0.95, 0.99]:\n",
    "    for beta2 in [0.99, 0.9999]:\n",
    "        for epsilon in [1e-9, 1e-4]:\n",
    "            \n",
    "            print(f\"beta1: {beta1}\\tbeta2: {beta2}\\tepsilon: {epsilon}\")\n",
    "            \n",
    "            \"\"\" Call SGD\n",
    "            w, b, *statistics = SGD(w, b, trainData, trainTarget, \n",
    "                            alpha=0.001, \n",
    "                            epochs=700, \n",
    "                            reg=0,\n",
    "                            lossType='CE',\n",
    "                            batch_size=500,\n",
    "                            **VTDatasets)\n",
    "            \"\"\"\n",
    "            train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "    \n",
    "            print(f\"Training loss: {train_loss[-1]:.4f}{'':20s}Training acc: {train_acc[-1]*100:.2f}%\")\n",
    "            print(f\"Validation loss: {valid_loss[-1]:.4f}{'':18s}Validation acc: {valid_acc[-1]*100:.2f}%\")\n",
    "            print(f\"Testing loss: {test_loss[-1]:.4f}{'':21s}Testing acc: {test_acc[-1]*100:.2f}%\")\n",
    "\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "            plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "            plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Comparison against Batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sandra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
