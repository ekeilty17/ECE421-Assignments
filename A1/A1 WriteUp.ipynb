{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given by the assignment\n",
    "def loadData():\n",
    "    with np.load('notMNIST.npz') as data :\n",
    "        Data, Target = data['images'], data['labels']\n",
    "        posClass = 2\n",
    "        negClass = 9\n",
    "        dataIndx = (Target==posClass) + (Target==negClass)\n",
    "        Data = Data[dataIndx]/255.\n",
    "        Target = Target[dataIndx].reshape(-1, 1)\n",
    "        Target[Target==posClass] = 1\n",
    "        Target[Target==negClass] = 0\n",
    "        np.random.seed(421)\n",
    "        randIndx = np.arange(len(Data))\n",
    "        np.random.shuffle(randIndx)\n",
    "        Data, Target = Data[randIndx], Target[randIndx]\n",
    "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "        testData, testTarget = Data[3600:], Target[3600:]\n",
    "    return trainData, validData, testData, trainTarget, validTarget, testTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: (3500, 28, 28)\tTraining tagets: (3500, 1)\n",
      "Validation Data: (100, 28, 28)\tValidation tagets: (100, 1)\n",
      "Testing Data: (145, 28, 28)\tTesting tagets:(145, 1)\n"
     ]
    }
   ],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "print(f\"Training Data: {trainData.shape}\\tTraining tagets: {trainTarget.shape}\")\n",
    "print(f\"Validation Data: {validData.shape}\\tValidation tagets: {validTarget.shape}\")\n",
    "print(f\"Testing Data: {testData.shape}\\tTesting tagets:{testTarget.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(image, target):\n",
    "    plt.imshow(image, cmap=\"hot\")\n",
    "    plt.title('J' if target == 0 else 'C')\n",
    "    # targets are binary encoded 0 == 'J' and 1 == 'C'\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(trainData[92], trainTarget[92])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Linear Regression\n",
    "### 1. Loss Function and Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error Loss\n",
    "\n",
    "# TODO: make x and W augemnted with b and 1s respectively\n",
    "def MSE(W, b, x, y, reg):\n",
    "    # L = 1/N[ ||Xw - y||^2 + \\lambda * ||w||^2 ]\n",
    "    N = y.shape[0]\n",
    "    return 1.0/N * (np.linalg.norm(x.dot(W) + b - y)**2 + reg * (np.linalg.norm(W)**2 + b**2)\n",
    "\n",
    "def gradMSE(W, b, x, y, reg):\n",
    "    # dL = 2/N[ X^T (Xw - y) + \\lambda w ]\n",
    "    N = y.shape[0]\n",
    "    return 2.0/N * ( x.T.dot( x.dot(W) + b - y ) + reg * W )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent_MSE(W, b, x, y, alpha, epochs, reg, error_tol):\n",
    "    for e in range(epochs):\n",
    "        for batch in batch:\n",
    "            # do something \n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tuning the Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to plot loss and accuracy\n",
    "def plot_loss(x, train_loss=None, valid_loss=None, test_loss=None, title=None):\n",
    "    if train_loss != None:\n",
    "        plt.plot(x, train_loss, label=\"Training Loss\")\n",
    "    if valid_loss != None:\n",
    "        plt.plot(x, valid_loss, label=\"Validation Loss\")\n",
    "    if test_loss != None:\n",
    "        plt.plot(x, test_loss, label=\"Testing Loss\")\n",
    "    \n",
    "    if title == None:\n",
    "        plt.title(\"Training Loss\")\n",
    "    else:\n",
    "        plt.title(title)\n",
    "    \n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.xlim(left=0)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def plot_accuracy(x, train_accuracy=None, valid_accuracy=None, test_accuracy=None, title=None):\n",
    "    if train_accuracy != None:\n",
    "        plt.plot(x, train_accuracy, label=\"Training Accuracy\")\n",
    "    if valid_accuracy != None:\n",
    "        plt.plot(x, valid_accuracy, label=\"Validation Accuracy\")\n",
    "    if test_accuracy != None:\n",
    "        plt.plot(x, test_accuracy, label=\"Testing Accuracy\")\n",
    "    \n",
    "    if title == None:\n",
    "        plt.title(\"Accuracy\")\n",
    "    else:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.xlim(left=0)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "    plt.grid(linestyle='-', axis='y')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation of Gradient Descent with 5000 epochs and \\lambda = 0. Investigate the\n",
    "# impact of learning rate, \\alpha = 0.005, 0.001, 0.0001 on the performance of your classifier. \n",
    "# Plot the training, validation and test losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate impact by modifying the regularization parameter, \\lambda = {0.001, 0.1, 0.5}. \n",
    "# Plot the training, validation and test loss for \\alpha = 0:005 and report the final training, \n",
    "# validation and test accuracy of your classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Comparing Batch GD with normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(X, y):\n",
    "    return np.linalg.inv( np.dot(X.T, X) ).dot(X.T).dot(y)\n",
    "# compare above to gradient descent solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Logistic Regression\n",
    "## 2.1 Binary cross-entropy loss\n",
    "### 1. Loss Function and Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will work for both scalar and vector z\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "# Cross Entropy Loss\n",
    "def crossEntropyLoss(W, b, x, y, reg):\n",
    "    y_hat = sigmoid(x.dot(W) + b)\n",
    "    L = np.vectorize(lambda x,y: -np.log(x) if y == 1 else -np.log(1-x))(y_hat, y)\n",
    "    #... not finished\n",
    "\n",
    "def gradCE(W, b, x, y, reg):\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(W, b, x, y, alpha, epochs, reg, error_tol, lossType=\"MSE\"):\n",
    "    if lossType == \"MSE\":\n",
    "        return grad_descent_MSE(W, b, x, y, alpha, epochs, reg, error_tol)\n",
    "    elif lossType == \"CE\":\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Variable 'lossType' must be either 'MSE' or 'CE'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Comparision to Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For zero weight decay, learning rate of 0.005 and 5000 epochs, \n",
    "# plot the training cross entropy loss and MSE loss for \n",
    "# logistic regression and linear regression respectively.\n",
    "# Comment on the effect of cross-entropy loss convergence behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Batch Gradient Descent vs. SGD and Adam\n",
    "## 3.1 SGD\n",
    "### 1. Building the Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildGraph(loss=\"MSE\"):\n",
    "    #Initialize weight and bias tensors\n",
    "    tf.set_random_seed(421)\n",
    "\n",
    "    if loss == \"MSE\":\n",
    "        # Your implementation\n",
    "        pass\n",
    "    elif loss == \"CE\":\n",
    "        #Your implementation here\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Variable 'lossType' must be either 'MSE' or 'CE'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementing Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the SGD algorithm for a minibatch size of 500 \n",
    "# optimizing over 700 epochs 2, minimizing the MSE (you will repeat this for the CE later).\n",
    "# Calculate the total number of batches required by dividing the number\n",
    "# of training instances by the minibatch size. After each epoch you will need to reshuffle the\n",
    "# training data and start sampling from the beginning again. Initially, set \\lambda = 0 and continue\n",
    "# to use the same \\alpha value (i.e. 0.001). After each epoch, store the training, validation and test\n",
    "# losses and accuracies. Use these to plot the loss and accuracy curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Batch Size Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hyperparameter Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Cross Entropy Loss Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Comparison against Batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
