{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given by the assignment\n",
    "def loadData():\n",
    "    with np.load('notMNIST.npz') as data :\n",
    "        Data, Target = data['images'], data['labels']\n",
    "        posClass = 2\n",
    "        negClass = 9\n",
    "        dataIndx = (Target==posClass) + (Target==negClass)\n",
    "        Data = Data[dataIndx]/255.\n",
    "        Target = Target[dataIndx].reshape(-1, 1)\n",
    "        Target[Target==posClass] = 1\n",
    "        Target[Target==negClass] = 0\n",
    "        np.random.seed(421)\n",
    "        randIndx = np.arange(len(Data))\n",
    "        np.random.shuffle(randIndx)\n",
    "        Data, Target = Data[randIndx], Target[randIndx]\n",
    "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "        testData, testTarget = Data[3600:], Target[3600:]\n",
    "    return trainData, validData, testData, trainTarget, validTarget, testTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Visualizing Dataset\n",
    "\n",
    "It's always important to visualize the dataset to gain and understanding of what the model is trying to accomplish. This can help in the debugging phase. The shape of each data set is printed out as well as a random sample of the training data is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: (3500, 28, 28)\tTraining tagets: (3500, 1)\n",
      "Validation Data: (100, 28, 28)\tValidation tagets: (100, 1)\n",
      "Testing Data: (145, 28, 28)\tTesting tagets:(145, 1)\n"
     ]
    }
   ],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "print(f\"Training Data: {trainData.shape}\\tTraining tagets: {trainTarget.shape}\")\n",
    "print(f\"Validation Data: {validData.shape}\\tValidation tagets: {validTarget.shape}\")\n",
    "print(f\"Testing Data: {testData.shape}\\tTesting tagets:{testTarget.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(image, target, ax=None):\n",
    "    ax = plt.gca() if ax == None else ax\n",
    "    ax.imshow(image, cmap=\"hot\")\n",
    "    ax.set_title('J' if target == 0 else 'C')\n",
    "    # targets are binary encoded 0 == 'J' and 1 == 'C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axis = plt.subplots(2, 5, figsize=(16, 5))\n",
    "for ax in axis.reshape(-1):\n",
    "    r = np.random.randint(trainData.shape[0])\n",
    "    plot(trainData[r], trainTarget[r], ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful functions that will be used throughout the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(X, w, b):\n",
    "    # flatten X\n",
    "    if len(X.shape) == 3:\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "    \n",
    "    # insert 1's at position 0 along the columns\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    # insert b at the front of W\n",
    "    w = np.insert(w, 0, b, axis=0)\n",
    "    \n",
    "    return X, w\n",
    "\n",
    "def get_zero_parameters():\n",
    "    w = np.zeros(d)\n",
    "    b = np.zeros(1)\n",
    "    return w, b\n",
    "\n",
    "def get_random_parameters():\n",
    "    w = np.random.uniform(low=-1.0, high=1.0, size=(d,))\n",
    "    b = np.random.uniform(low=-1.0, high=1.0, size=(1,))\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    return X.dot(w) + b\n",
    "\n",
    "def accuracy(w, b, X, y):\n",
    "    y = y.reshape(-1)\n",
    "    y_pred = predict(w, b, X)\n",
    "    y_pred = np.vectorize(lambda z: 1 if z > 0 else 0)(y_pred)\n",
    "    return sum(y_pred == y) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(x, train_loss=None, valid_loss=None, test_loss=None, title=None, ax=None):\n",
    "    ax = plt.gca() if ax == None else ax\n",
    "    if train_loss != None:\n",
    "        ax.plot(x, train_loss, label=\"Training Loss\")\n",
    "    if valid_loss != None:\n",
    "        ax.plot(x, valid_loss, label=\"Validation Loss\")\n",
    "    if test_loss != None:\n",
    "        ax.plot(x, test_loss, label=\"Testing Loss\")\n",
    "    \n",
    "    if title == None:\n",
    "        ax.set_title(\"Loss\")\n",
    "    else:\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    ax.set_xlabel(\"Iterations\")\n",
    "    ax.set_xlim(left=0)\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "def plot_accuracy(x, train_accuracy=None, valid_accuracy=None, test_accuracy=None, title=None, ax=None):\n",
    "    ax = plt.gca() if ax == None else ax\n",
    "    if train_accuracy != None:\n",
    "        ax.plot(x, train_accuracy, label=\"Training Accuracy\")\n",
    "    if valid_accuracy != None:\n",
    "        ax.plot(x, valid_accuracy, label=\"Validation Accuracy\")\n",
    "    if test_accuracy != None:\n",
    "        ax.plot(x, test_accuracy, label=\"Testing Accuracy\")\n",
    "    \n",
    "    if title == None:\n",
    "        ax.set_title(\"Accuracy\")\n",
    "    else:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    ax.set_xlabel(\"Iterations\")\n",
    "    ax.set_xlim(left=0)\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_yticks(np.arange(0, 1.1, step=0.1))\n",
    "    ax.grid(linestyle='-', axis='y')\n",
    "    ax.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some global variables that will be used throughout the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "VTDatasets = {\"validData\" : validData, \"validTarget\" : validTarget,\n",
    "              \"testData\" : testData, \"testTarget\" : testTarget}\n",
    "\n",
    "N = trainData.shape[0]\n",
    "d = trainData.shape[1] * trainData.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Linear Regression\n",
    "### 1. Loss Function and Gradient\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(n)} = W^T \\textbf{x}^{(n)} + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{MSE} = \\frac{1}{N}\\sum_{n=1}^N(\\hat{y}^{(n)} - y^{(n)})^2 + \\lambda \\Vert W \\Vert_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{MSE}}{\\partial b} = \\frac{2}{N} \\sum_{n=1}^N (\\hat{y}^{(n)} - y^{(n)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{MSE}}{\\partial W} = \\frac{2}{N} \\ X^T \\left ( \\hat{\n",
    "\\textbf{y}} - \\textbf{y} \\right ) + \\lambda W\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(w, b, X, y, reg):\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    y = y.reshape(-1)\n",
    "    return np.square(X.dot(w) + b - y).mean() + reg * np.square(w).sum()\n",
    "\n",
    "def gradMSE(w, b, X, y, reg):\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    y = y.reshape(-1)\n",
    "    N = y.shape[0]\n",
    "    \n",
    "    w_grad = 2.0/N * X.T.dot(X.dot(w) + b - y) + reg * w\n",
    "    b_grad = 2.0/N * np.sum(X.dot(w) + b - y)\n",
    "    return w_grad, b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent_MSE(w, b, X, y, alpha, epochs, reg, error_tol=1e-7, \n",
    "                     validData=None, validTarget=None, testData=None, testTarget=None):\n",
    "    train_loss, train_acc = [], []\n",
    "    valid_loss, valid_acc = [], []\n",
    "    test_loss, test_acc = [], []\n",
    "    for i in range(epochs):\n",
    "        grad_w, grad_b = gradMSE(w, b, X, y, reg)\n",
    "        w -= alpha * grad_w\n",
    "        b -= alpha * grad_b\n",
    "        \n",
    "        # Calculating Statistics\n",
    "        train_loss.append( MSE(w, b, X, y, reg) )\n",
    "        train_acc.append( accuracy(w, b, X, y) )\n",
    "\n",
    "        if not validData is None and not validTarget is None:\n",
    "            valid_loss.append( MSE(w, b, validData, validTarget, reg) )\n",
    "            valid_acc.append( accuracy(w, b, validData, validTarget) )\n",
    "        if not testData is None and not testTarget is None:\n",
    "            test_loss.append( MSE(w, b, testData, testTarget, reg) )\n",
    "            test_acc.append( accuracy(w, b, testData, testTarget) )\n",
    "\n",
    "        # Check stopping condition\n",
    "        if i > 1 and np.abs(train_loss[-2] - train_loss[-1]) <= error_tol:\n",
    "            break\n",
    "\n",
    "    statistics = (train_loss, train_acc)\n",
    "    if not validData is None and not validTarget is None:\n",
    "        statistics += (valid_loss, valid_acc, )\n",
    "    if not testData is None and not testTarget is None:\n",
    "        statistics += (test_loss, test_acc,)\n",
    "    # Python 3.8 made this easier, but 3.7 you have to do this\n",
    "    out = (w, b, *statistics)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tuning the Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f50a13dede4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                          \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                                          \u001b[0mreg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                                          **VTDatasets)\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-da9bdb4a1ceb>\u001b[0m in \u001b[0;36mgrad_descent_MSE\u001b[1;34m(w, b, X, y, alpha, epochs, reg, error_tol, validData, validTarget, testData, testTarget)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mvalid_acc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidTarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtestData\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtestTarget\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mtest_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestTarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mtest_acc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestTarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-474d4252e176>\u001b[0m in \u001b[0;36mMSE\u001b[1;34m(w, b, X, y, reg)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreg\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgradMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test your implementation of Gradient Descent with 5000 epochs and \\lambda = 0. Investigate the\n",
    "# impact of learning rate, \\alpha = 0.005, 0.001, 0.0001 on the performance of your classifier. \n",
    "# Plot the training, validation and test losses.\n",
    "\n",
    "for alpha in [0.005, 0.001, 0.0001]:\n",
    "    \n",
    "    print(\"alpha =\", alpha)\n",
    "    \n",
    "    w, b = get_random_parameters()\n",
    "    w, b, *statistics = grad_descent_MSE(w, b, trainData, trainTarget, \n",
    "                                         alpha=alpha, \n",
    "                                         epochs=5000, \n",
    "                                         reg=0, \n",
    "                                         **VTDatasets)\n",
    "    train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "    \n",
    "    print(f\"Training loss: {train_loss[-1]:.4f}{'':20s}Training acc: {train_acc[-1]*100:.2f}%\")\n",
    "    print(f\"Validation loss: {valid_loss[-1]:.4f}{'':18s}Validation acc: {valid_acc[-1]*100:.2f}%\")\n",
    "    print(f\"Testing loss: {test_loss[-1]:.4f}{'':21s}Testing acc: {test_acc[-1]*100:.2f}%\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "    plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization = 0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-52cda818a0b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                          \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                                          \u001b[0mreg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                                          **VTDatasets)\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-da9bdb4a1ceb>\u001b[0m in \u001b[0;36mgrad_descent_MSE\u001b[1;34m(w, b, X, y, alpha, epochs, reg, error_tol, validData, validTarget, testData, testTarget)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# Calculating Statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mtrain_acc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvalidData\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvalidTarget\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-7b84c466834f>\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(w, b, X, y)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-7b84c466834f>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(w, b, X)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Investigate impact by modifying the regularization parameter, \\lambda = {0.001, 0.1, 0.5}. \n",
    "# Plot the training, validation and test loss for \\alpha = 0:005 and report the final training, \n",
    "# validation and test accuracy of your classifier.\n",
    "\n",
    "for reg in [0.001, 0.1, 0.5]:\n",
    "    \n",
    "    print(\"regularization =\", reg)\n",
    "    \n",
    "    w, b = get_random_parameters()\n",
    "    w, b, *statistics = grad_descent_MSE(w, b, trainData, trainTarget, \n",
    "                                         alpha=0.005, \n",
    "                                         epochs=5000, \n",
    "                                         reg=reg, \n",
    "                                         **VTDatasets)\n",
    "    train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "    \n",
    "    print(f\"Training loss: {train_loss[-1]:.4f}{'':.20s}\\tTraining acc: {train_acc[-1]*100:.2f}%\")\n",
    "    print(f\"Validation loss: {valid_loss[-1]:.4f}\\tValidation acc: {valid_acc[-1]*100:.2f}%\")\n",
    "    print(f\"Testing loss: {test_loss[-1]:.4f}\\tTesting acc: {test_acc[-1]*100:.2f}%\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "    plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Comparing Batch GD with normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(X, y):\n",
    "    N = X.shape[0]\n",
    "    d = X.shape[1] * X.shape[2]\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    y = y.reshape(-1)\n",
    "    \n",
    "    # overparameterized (deep learning)\n",
    "    if N < d:\n",
    "        w_aug = X.T @ np.linalg.inv( X @ X.T ) @ y\n",
    "    \n",
    "    # underparameterized (typical case)\n",
    "    else:\n",
    "        w_aug = np.linalg.inv( X.T @ X ) @ X.T @ y\n",
    "    \n",
    "    return w_aug[1:], w_aug[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare above to gradient descent solution\n",
    "w_LS, b_LS = least_squares(trainData, trainTarget)\n",
    "\n",
    "loss = MSE(w_LS, b_LS, trainData, trainTarget, 0)\n",
    "acc = accuracy(w_LS, b_LS, trainData, trainTarget)\n",
    "print(f\"Least Squares Training loss: {loss:.4f}\\tLeast Squares Training acc: {100*acc:.2f}%\")\n",
    "loss = MSE(w_LS, b_LS, validData, validTarget, 0)\n",
    "acc = accuracy(w_LS, b_LS, validData, validTarget)\n",
    "print(f\"Least Squares Validation loss: {loss:.4f}\\tLeast Squares Validation acc: {100*acc:.2f}%\")\n",
    "loss = MSE(w_LS, b_LS, testData, testTarget, 0)\n",
    "acc = accuracy(w_LS, b_LS, testData, testTarget)\n",
    "print(f\"Least Squares Testing loss: {loss:.4f}\\tLeast Squares Testing acc: {100*acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the analytical solution, the training loss achieved with the analytical equation is 0.0187 with a training accuracy of 71.29%. The training loss and accuracies for batch gradient descent were respectively 0.6918 and 67.97%. From the values, we see that the analytical solution performed better. However, computing it grows increasingly difficult with the size of the problem. As the problem scales, batch gradient descent allows for faster convergence with comparable accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Logistic Regression\n",
    "## 2.1 Binary cross-entropy loss\n",
    "### 1. Loss Function and Gradient\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(n)} = \\sigma(W^T\\textbf{x}^{(n)} + b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{CE} = \\frac{1}{N}\\sum_{n=1}^{N} \\left [ -y^{(n)} \\log(\\hat{y}^{(n)}) -(1- y^{(n)}) \\log (1 - \\hat{y}^{(n)} ) \\right ] + \\frac{\\lambda}{2} \\Vert W \\Vert^2_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{CE}}{\\partial b} = \\frac{1}{N} \\sum_{n=1}^{N} \\left [ \\hat{y}^{(n)} - y^{(n)} \\right ]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{CE}}{\\partial W} = \\frac{1}{N} \\sum_{n=1}^{N} \\left [ (\\hat{y}^{(n)} - y^{(n)}) \\ \\textbf{x}^{(n)} \\right ] + \\lambda W\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "    \n",
    "def crossEntropyLoss(w, b, X, y, reg):    \n",
    "    X, w = augment(X, w, b)\n",
    "    y = y.reshape(-1)\n",
    "    N = y.shape[0]\n",
    "    \n",
    "    y_hat = sigmoid(X.dot(w))\n",
    "    \n",
    "    return 1.0/N * (-y.dot(np.log(y_hat+1e-20)) - (1 - y).dot(np.log(1 - y_hat+1e-20))) + reg/2.0 * np.square(w[1:]).sum()\n",
    "    \n",
    "def gradCE(w, b, X, y, reg):\n",
    "    X, w = augment(X, w, b)\n",
    "    y = y.reshape(-1)\n",
    "    N = y.shape[0]\n",
    "    \n",
    "    y_hat = sigmoid(X.dot(w))\n",
    "    \n",
    "    grad = 1.0 /N * X.T.dot(y_hat - y) + reg * w\n",
    "    \n",
    "    return grad[1:], grad[0] - reg * w[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(w, b, X, y, alpha, epochs, reg, error_tol=1e-7, lossType=\"MSE\", \n",
    "                 validData=None, validTarget=None, testData=None, testTarget=None):\n",
    "    loss_func, grad_func = None, None\n",
    "    if lossType == \"MSE\":\n",
    "        loss_func, grad_func = MSE, gradMSE\n",
    "    elif lossType == \"CE\":\n",
    "        loss_func, grad_func = crossEntropyLoss, gradCE\n",
    "    else:\n",
    "        raise ValueError(\"Variable 'lossType' must be either 'MSE' or 'CE'.\")\n",
    "    \n",
    "    train_loss, train_acc = [], []\n",
    "    valid_loss, valid_acc = [], []\n",
    "    test_loss, test_acc = [], []\n",
    "    printing = False\n",
    "    for i in range(epochs):\n",
    "        grad_w, grad_b = grad_func(w, b, X, y, reg)\n",
    "        w -= alpha * grad_w\n",
    "        b -= alpha * grad_b\n",
    "\n",
    "        # Calculating Statistics\n",
    "        train_loss.append(loss_func(w, b, X, y, reg))\n",
    "        train_acc.append(accuracy(w, b, X, y))\n",
    "\n",
    "        if not validData is None and not validTarget is None:\n",
    "            valid_loss.append(loss_func(w, b, validData, validTarget, reg))\n",
    "            valid_acc.append(accuracy(w, b, validData, validTarget))\n",
    "        if not testData is None and not testTarget is None:\n",
    "            test_loss.append(loss_func(w, b, testData, testTarget, reg))\n",
    "            test_acc.append(accuracy(w, b, testData, testTarget))\n",
    "\n",
    "        # Check stopping condition\n",
    "        if i > 1 and np.abs(train_loss[-2] - train_loss[-1]) <= error_tol:\n",
    "            break\n",
    "\n",
    "    statistics = (train_loss, train_acc)\n",
    "    if not validData is None and not validTarget is None:\n",
    "        statistics += (valid_loss, valid_acc,)\n",
    "    if not testData is None and not testTarget is None:\n",
    "        statistics += (test_loss, test_acc,)\n",
    "    out = (w, b, *statistics)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = get_random_parameters()\n",
    "w, b, *statistics = grad_descent(w, b, trainData, trainTarget, \n",
    "                                 alpha=0.005, \n",
    "                                 epochs=5000, \n",
    "                                 reg=0.1,\n",
    "                                 lossType='CE',\n",
    "                                 **VTDatasets)\n",
    "train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "\n",
    "print(f\"Training loss: {train_loss[-1]:.4f}{'':.20s}\\tTraining acc: {train_acc[-1]*100:.2f}%\")\n",
    "print(f\"Validation loss: {valid_loss[-1]:.4f}\\tValidation acc: {valid_acc[-1]*100:.2f}%\")\n",
    "print(f\"Testing loss: {test_loss[-1]:.4f}\\tTesting acc: {test_acc[-1]*100:.2f}%\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Comparision to Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For zero weight decay, learning rate of 0.005 and 5000 epochs, \n",
    "# plot the training cross entropy loss and MSE loss for \n",
    "# logistic regression and linear regression respectively.\n",
    "# Comment on the effect of cross-entropy loss convergence behaviour.\n",
    "\n",
    "w, b = get_random_parameters()\n",
    "\n",
    "# Linear Regresesion\n",
    "print(\"Linear Regression\")\n",
    "w_lin, b_lin, *statistics = grad_descent(w, b, trainData, trainTarget, \n",
    "                                         alpha=0.005, \n",
    "                                         epochs=5000, \n",
    "                                         reg=0,\n",
    "                                         lossType='MSE',\n",
    "                                         **VTDatasets)\n",
    "train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "\n",
    "print(f\"Training loss: {train_loss[-1]:.4f}{'':.20s}\\tTraining acc: {train_acc[-1]*100:.2f}%\")\n",
    "print(f\"Validation loss: {valid_loss[-1]:.4f}\\tValidation acc: {valid_acc[-1]*100:.2f}%\")\n",
    "print(f\"Testing loss: {test_loss[-1]:.4f}\\tTesting acc: {test_acc[-1]*100:.2f}%\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"Logistic Regression\")\n",
    "w_log, b_log, *statistics = grad_descent(w, b, trainData, trainTarget, \n",
    "                                         alpha=0.005, \n",
    "                                         epochs=5000, \n",
    "                                         reg=0,\n",
    "                                         lossType='CE',\n",
    "                                         **VTDatasets)\n",
    "train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "\n",
    "print(f\"Training loss: {train_loss[-1]:.4f}{'':.20s}\\tTraining acc: {train_acc[-1]*100:.2f}%\")\n",
    "print(f\"Validation loss: {valid_loss[-1]:.4f}\\tValidation acc: {valid_acc[-1]*100:.2f}%\")\n",
    "print(f\"Testing loss: {test_loss[-1]:.4f}\\tTesting acc: {test_acc[-1]*100:.2f}%\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "plt.show()\n",
    "\n",
    "# Sandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Batch Gradient Descent vs. SGD and Adam\n",
    "## 3.1 SGD\n",
    "### 1. Building the Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildGraph(beta1=None, beta2=None, epsilon=None, alpha=0.001, loss=\"MSE\", seed=421, regulizer=5):\n",
    "    # Initialize weight and bias tensors\n",
    "    tf.set_random_seed(seed)\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    \n",
    "\n",
    "    # Getting small random initial values for weights and bias\n",
    "    w = tf.truncated_normal([d, 1], stddev=0.5)\n",
    "    b = tf.truncated_normal([1, 1], stddev=0.5)\n",
    "\n",
    "    # Converting into tensorflow Variable objects\n",
    "    w = tf.Variable(w, name=\"weights\")\n",
    "    b = tf.Variable(b, name=\"bias\")\n",
    "\n",
    "    # tensorflow objects for data\n",
    "    X = tf.placeholder(tf.float32, (None, d))\n",
    "    y = tf.placeholder(tf.float32, (None, 1))\n",
    "    reg = regulizer\n",
    "\n",
    "    y_pred = None\n",
    "    loss_tensor = None\n",
    "    if loss == \"MSE\":\n",
    "        y_pred = tf.add(tf.matmul(X, w), b)\n",
    "        loss_tensor = tf.losses.mean_squared_error(y, y_pred)\n",
    "    elif loss == \"CE\":\n",
    "        y_pred = tf.sigmoid(tf.matmul(X, w) + b)\n",
    "        loss_tensor = tf.losses.sigmoid_cross_entropy(y, y_pred)\n",
    "    else:\n",
    "        raise ValueError(\"Variable 'lossType' must be either 'MSE' or 'CE'.\")\n",
    "\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=alpha, beta1=beta1, beta2=beta2, epsilon=epsilon)\n",
    "    opt_op = opt.minimize(loss_tensor)\n",
    "\n",
    "    return w, b, reg, X, y, y_pred, loss_tensor, opt_op\n",
    "\n",
    "# test call to see output\n",
    "# buildGraph(trainData, trainTarget, beta1=0.95, beta2=0.99, epsilon=10^-5, alpha=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementing Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005 cost= 49489.910156250\n",
      "Epoch: 0010 cost= 66290.210937500\n",
      "Epoch: 0015 cost= 59306.003906250\n",
      "Epoch: 0020 cost= 57412.445312500\n",
      "Epoch: 0025 cost= 62505.382812500\n",
      "Epoch: 0030 cost= 56685.679687500\n",
      "Epoch: 0035 cost= 53517.511718750\n",
      "Epoch: 0040 cost= 49155.593750000\n",
      "Epoch: 0045 cost= 55604.511718750\n",
      "Epoch: 0050 cost= 1220774.750000000\n",
      "Epoch: 0055 cost= 1100649.750000000\n",
      "Epoch: 0060 cost= 1040965.500000000\n",
      "Epoch: 0065 cost= 1010401.250000000\n",
      "Epoch: 0070 cost= 988882.250000000\n",
      "Epoch: 0075 cost= 970160.500000000\n",
      "Epoch: 0080 cost= 952748.687500000\n",
      "Epoch: 0085 cost= 936163.500000000\n",
      "Epoch: 0090 cost= 920173.812500000\n",
      "Epoch: 0095 cost= 904682.312500000\n",
      "Epoch: 0100 cost= 889638.750000000\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "display_step = 5\n",
    "\n",
    "w, b, reg, data, labels, pred_labels, loss_tensor, opt_op = buildGraph(beta1=0.95, beta2=0.99, epsilon=10^-5, alpha=0.01, loss=\"MSE\")\n",
    "X = trainData.reshape(trainData.shape[0], -1)\n",
    "Y = trainTarget\n",
    "batch_iter = BatchLoader((X, Y), batch_size=500, randomize=False)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in batch_iter:\n",
    "            sess.run(opt_op, feed_dict={data: x, labels: y})\n",
    "\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            c = sess.run(loss_tensor, feed_dict={data: x, labels: y})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader(object):\n",
    "\n",
    "    def __init__(self, data, batch_size=None, randomize=True, drop_last=False, seed=None):\n",
    "    \n",
    "        # error checking\n",
    "        if len(data) > 1:\n",
    "            for i in range(len(data)-1):\n",
    "                if data[i].shape[0] != data[i+1].shape[0]:\n",
    "                    raise ValueError(\"All inputs must have the same number of elements\")\n",
    "    \n",
    "        self.data = data if type(data) == tuple else (data, )\n",
    "        self.N = data[0].shape[0]\n",
    "        self.batch_size = batch_size if batch_size != None else self.N\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        # shuffling data\n",
    "        if randomize:\n",
    "            indices = np.arange(self.N)\n",
    "            np.random.seed(seed)\n",
    "            np.random.shuffle(indices)\n",
    "            self.data = tuple([d[indices] for d in self.data])\n",
    "    \n",
    "        self.index = 0 \n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "    \n",
    "        # stop condition\n",
    "        if self.index >= self.N:\n",
    "            self.index = 0          # resetting index for next iteration\n",
    "            raise StopIteration\n",
    "\n",
    "        # iterating\n",
    "        self.index += self.batch_size\n",
    "    \n",
    "        if self.index > self.N:\n",
    "            if self.drop_last:\n",
    "                self.index = 0      # resetting index for next iteration\n",
    "                raise StopIteration\n",
    "            else:\n",
    "                return tuple([ d[self.index - self.batch_size: ] for d in self.data ])\n",
    "        else:\n",
    "            return tuple([ d[self.index - self.batch_size: self.index] for d in self.data ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(X, y, alpha, epochs, reg, error_tol=1e-7, lossType=\"CE\",\n",
    "              batch_size=500, randomize=True, beta1=None, beta2=None, epsilon=None,\n",
    "              validData=None, validTarget=None, testData=None, testTarget=None):\n",
    "    \n",
    "    optimizer, *_ = buildGraph(beta1=beta1, beta2=beta2, epsilon=epsilon, \n",
    "                               alpha=alpha, loss=lossType)\n",
    "    \n",
    "    train_loss, train_acc = [], []\n",
    "    valid_loss, valid_acc = [], []\n",
    "    test_loss, test_acc = [], []\n",
    "    printing = False\n",
    "    \n",
    "    batch_iter = BatchLoader((X, y), batch_size=batch_size, randomize=randomize)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        for batch, targets in batch_iter:\n",
    "            \n",
    "            # do the update using the optimizer\n",
    "            \n",
    "            # Calculating Statistics\n",
    "            running_loss += loss_func(w, b, batch, targets, reg) * batch.shape[0]\n",
    "            running_acc += accuracy(w, b, batch, targets) * batch.shape[0]\n",
    "            \n",
    "            # Check stopping condition\n",
    "            if i > 1 and np.abs(train_loss[-2] - train_loss[-1]) <= error_tol:\n",
    "                break\n",
    "        else:\n",
    "            \"\"\"All of this will probably need to change slightly\"\"\"\n",
    "            # Calculating Statistics\n",
    "            train_loss.append(running_loss / X.shape[0])\n",
    "            train_acc.append(running_acc / X.shape[0])\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            \n",
    "            if not validData is None and not validTarget is None:\n",
    "                valid_loss.append(loss_func(w, b, validData, validTarget, reg))\n",
    "                valid_acc.append(accuracy(w, b, validData, validTarget))\n",
    "            if not testData is None and not testTarget is None:\n",
    "                test_loss.append(loss_func(w, b, testData, testTarget, reg))\n",
    "                test_acc.append(accuracy(w, b, testData, testTarget))\n",
    "            \n",
    "            continue\n",
    "        break\n",
    "\n",
    "    statistics = (train_loss, train_acc)\n",
    "    if not validData is None and not validTarget is None:\n",
    "        statistics += (valid_loss, valid_acc,)\n",
    "    if not testData is None and not testTarget is None:\n",
    "        statistics += (test_loss, test_acc,)\n",
    "    out = (w, b, *statistics)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "tf.placeholder() is not compatible with eager execution.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-171b80ff92fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                         \u001b[0mlossType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'MSE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                         **VTDatasets)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-7db2e3e0763c>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(X, y, alpha, epochs, reg, error_tol, lossType, batch_size, randomize, beta1, beta2, epsilon, validData, validTarget, testData, testTarget)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     optimizer, *_ = buildGraph(beta1=beta1, beta2=beta2, epsilon=epsilon, \n\u001b[0;32m----> 6\u001b[0;31m                                alpha=alpha, loss=lossType)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d5b53dd9c6fc>\u001b[0m in \u001b[0;36mbuildGraph\u001b[0;34m(beta1, beta2, epsilon, alpha, loss, seed)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# tensorflow objects for data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# X = tf.placeholder(tf.float32, (None, d))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ece421/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   2625\u001b[0m   \"\"\"\n\u001b[1;32m   2626\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2627\u001b[0;31m     raise RuntimeError(\"tf.placeholder() is not compatible with \"\n\u001b[0m\u001b[1;32m   2628\u001b[0m                        \"eager execution.\")\n\u001b[1;32m   2629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tf.placeholder() is not compatible with eager execution."
     ]
    }
   ],
   "source": [
    "# Implement the SGD algorithm for a minibatch size of 500\n",
    "# optimizing over 700 epochs, minimizing the MSE (you will repeat this for the CE later).\n",
    "# Calculate the total number of batches required by dividing the number\n",
    "# of training instances by the minibatch size. After each epoch you will need to reshuffle the\n",
    "# training data and start sampling from the beginning again. Initially, set \\lambda = 0 and continue\n",
    "# to use the same \\alpha value (i.e. 0.001). After each epoch, store the training, validation and test\n",
    "# losses and accuracies. Use these to plot the loss and accuracy curves.\n",
    "\n",
    "w, b = get_random_parameters()\n",
    "w, b, *statistics = SGD(trainData, trainTarget, \n",
    "                        alpha=0.001, \n",
    "                        epochs=700, \n",
    "                        reg=0,\n",
    "                        lossType='MSE',\n",
    "                        batch_size=500,\n",
    "                        **VTDatasets)\n",
    "train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Batch Size Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size in [100, 700, 1750]:\n",
    "    \n",
    "    print(\"batch size =\", batch_size)\n",
    "    \n",
    "    w, b = get_random_parameters()\n",
    "    w, b, *statistics = SGD(w, b, trainData, trainTarget, \n",
    "                            alpha=0.001, \n",
    "                            epochs=700, \n",
    "                            reg=0,\n",
    "                            lossType='CE',\n",
    "                            batch_size=500,\n",
    "                            **VTDatasets)\n",
    "    train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "    \n",
    "    print(f\"Training loss: {train_loss[-1]:.4f}{'':20s}Training acc: {train_acc[-1]*100:.2f}%\")\n",
    "    print(f\"Validation loss: {valid_loss[-1]:.4f}{'':18s}Validation acc: {valid_acc[-1]*100:.2f}%\")\n",
    "    print(f\"Testing loss: {test_loss[-1]:.4f}{'':21s}Testing acc: {test_acc[-1]*100:.2f}%\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "    plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hyperparameter Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "for beta1 in [0.95, 0.99]:\n",
    "    for beta2 in [0.99, 0.9999]:\n",
    "        for epsilon in [1e-9, 1e-4]:\n",
    "            \n",
    "            print(f\"beta1: {beta1}\\tbeta2: {beta2}\\tepsilon: {epsilon}\")\n",
    "            \n",
    "            \"\"\" Call SGD\n",
    "            w, b, *statistics = SGD(w, b, trainData, trainTarget, \n",
    "                            alpha=0.001, \n",
    "                            epochs=700, \n",
    "                            reg=0,\n",
    "                            lossType='MSE',\n",
    "                            batch_size=500,\n",
    "                            **VTDatasets)\n",
    "            \"\"\"\n",
    "            train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "    \n",
    "            print(f\"Training loss: {train_loss[-1]:.4f}{'':20s}Training acc: {train_acc[-1]*100:.2f}%\")\n",
    "            print(f\"Validation loss: {valid_loss[-1]:.4f}{'':18s}Validation acc: {valid_acc[-1]*100:.2f}%\")\n",
    "            print(f\"Testing loss: {test_loss[-1]:.4f}{'':21s}Testing acc: {test_acc[-1]*100:.2f}%\")\n",
    "\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "            plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "            plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Cross Entropy Loss Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1.2 with Cross Entropy Loss\n",
    "w, b = get_random_parameters()\n",
    "w, b, *statistics = SGD(w, b, trainData, trainTarget, \n",
    "                        alpha=0.001, \n",
    "                        epochs=700, \n",
    "                        reg=0,\n",
    "                        lossType='CE',\n",
    "                        batch_size=500,\n",
    "                        **VTDatasets)\n",
    "train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "plt.show()\n",
    "\n",
    "# 3.1.4 with Cross Entropy Loss\n",
    "for beta1 in [0.95, 0.99]:\n",
    "    for beta2 in [0.99, 0.9999]:\n",
    "        for epsilon in [1e-9, 1e-4]:\n",
    "            \n",
    "            print(f\"beta1: {beta1}\\tbeta2: {beta2}\\tepsilon: {epsilon}\")\n",
    "            \n",
    "            \"\"\" Call SGD\n",
    "            w, b, *statistics = SGD(w, b, trainData, trainTarget, \n",
    "                            alpha=0.001, \n",
    "                            epochs=700, \n",
    "                            reg=0,\n",
    "                            lossType='CE',\n",
    "                            batch_size=500,\n",
    "                            **VTDatasets)\n",
    "            \"\"\"\n",
    "            train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "    \n",
    "            print(f\"Training loss: {train_loss[-1]:.4f}{'':20s}Training acc: {train_acc[-1]*100:.2f}%\")\n",
    "            print(f\"Validation loss: {valid_loss[-1]:.4f}{'':18s}Validation acc: {valid_acc[-1]*100:.2f}%\")\n",
    "            print(f\"Testing loss: {test_loss[-1]:.4f}{'':21s}Testing acc: {test_acc[-1]*100:.2f}%\")\n",
    "\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "            plot_loss(np.arange(0, len(train_loss), 1), train_loss, valid_loss, test_loss, ax=ax[0])\n",
    "            plot_accuracy(np.arange(0, len(train_loss), 1), train_acc, valid_acc, test_acc, ax=ax[1])\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Comparison against Batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sandra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
