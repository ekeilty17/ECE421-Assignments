{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given by the assignment\n",
    "def loadData():\n",
    "    with np.load('notMNIST.npz') as data :\n",
    "        Data, Target = data['images'], data['labels']\n",
    "        posClass = 2\n",
    "        negClass = 9\n",
    "        dataIndx = (Target==posClass) + (Target==negClass)\n",
    "        Data = Data[dataIndx]/255.\n",
    "        Target = Target[dataIndx].reshape(-1, 1)\n",
    "        Target[Target==posClass] = 1\n",
    "        Target[Target==negClass] = 0\n",
    "        np.random.seed(421)\n",
    "        randIndx = np.arange(len(Data))\n",
    "        np.random.shuffle(randIndx)\n",
    "        Data, Target = Data[randIndx], Target[randIndx]\n",
    "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "        testData, testTarget = Data[3600:], Target[3600:]\n",
    "    return trainData, validData, testData, trainTarget, validTarget, testTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: (3500, 28, 28)\tTraining tagets: (3500, 1)\n",
      "Validation Data: (100, 28, 28)\tValidation tagets: (100, 1)\n",
      "Testing Data: (145, 28, 28)\tTesting tagets:(145, 1)\n"
     ]
    }
   ],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "print(f\"Training Data: {trainData.shape}\\tTraining tagets: {trainTarget.shape}\")\n",
    "print(f\"Validation Data: {validData.shape}\\tValidation tagets: {validTarget.shape}\")\n",
    "print(f\"Testing Data: {testData.shape}\\tTesting tagets:{testTarget.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(image, target):\n",
    "    plt.imshow(image, cmap=\"hot\")\n",
    "    plt.title('J' if target == 0 else 'C')\n",
    "    # targets are binary encoded 0 == 'J' and 1 == 'C'\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(trainData[0], trainTarget[0])\n",
    "plot(trainData[1], trainTarget[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(X, w, b):\n",
    "    # flatten X\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    # insert 1's at position 0 along the columns\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    # insert b at the front of W\n",
    "    w = np.insert(w, 0, b, axis=0)\n",
    "    \n",
    "    return X, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    X, w = augment(X, w, b)\n",
    "    return X.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, w, b, y):\n",
    "    y_pred = predict(X, w, b)\n",
    "    y = y.reshape(-1)\n",
    "    return sum(y_pred == y) / y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Linear Regression\n",
    "### 1. Loss Function and Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX = trainData\\ny = trainTarget\\nN = X.shape[0]\\nd = X.shape[1] * X.shape[2]\\n\\nw = np.random.random_sample(d)\\nb = np.random.random_sample(1)\\n\\ngradMSE(w, b, X, y, 0.1)\\n'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Squared Error Loss\n",
    "\n",
    "# TODO: make x and W augemnted with b and 1s respectively\n",
    "def MSE(w, b, X, y, reg):\n",
    "    X, w = augment(X, w, b)\n",
    "    y = y.reshape(-1)\n",
    "    return np.square(X.dot(w) - y).mean() + reg * np.square(w).sum()\n",
    "\n",
    "def gradMSE(w, b, X, y, reg):\n",
    "    X_aug, w_aug = augment(X, w, b)\n",
    "    y = y.reshape(-1)\n",
    "    \n",
    "    grad_b = 2 * (X.dot(w_aug) - y)\n",
    "    grad_w = 2 * X.T.dot(X_aug.dot(w_aug) - y) + reg * np.linalg.norm(w)\n",
    "    return grad_w, grad_b\n",
    "\n",
    "\"\"\"\n",
    "X = trainData\n",
    "y = trainTarget\n",
    "N = X.shape[0]\n",
    "d = X.shape[1] * X.shape[2]\n",
    "\n",
    "w = np.random.random_sample(d)\n",
    "b = np.random.random_sample(1)\n",
    "\n",
    "gradMSE(w, b, X, y, 0.1)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (784,) (28,28) (784,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-006899089920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mgrad_descent_MSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainTarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-108-006899089920>\u001b[0m in \u001b[0;36mgrad_descent_MSE\u001b[0;34m(w, b, X, y, alpha, epochs, reg, error_tol)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mgrad_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (784,) (28,28) (784,) "
     ]
    }
   ],
   "source": [
    "def grad_descent_MSE(w, b, X, y, alpha, epochs, reg, error_tol):\n",
    "    prev_loss = 0\n",
    "    printing = True\n",
    "    for i in range(epochs):\n",
    "        grad_w, grad_b = gradMSE(W, b, X, y, reg)\n",
    "        w -= alpha * grad_w\n",
    "        b -= alpha * grad_b\n",
    "        loss = MSE(w, b, X, y, reg)\n",
    "\n",
    "        # Print Losses if printing is on\n",
    "        if printing:\n",
    "            print(\"Training loss is\", loss)\n",
    "            print(\"Validation loss is \", MSE(w, b, validData, validTarget, reg))\n",
    "\n",
    "        # Check stopping condition\n",
    "        if np.abs(prev_loss - loss) <= error_tol:\n",
    "            break\n",
    "        else:\n",
    "            prev_loss = loss\n",
    "    print(\"Test loss is \", MSE(w, b, testData, testTarget, reg))\n",
    "    return w, b\n",
    "\n",
    "X = trainData\n",
    "N = X.shape[0]\n",
    "d = X.shape[1] * X.shape[2]\n",
    "\n",
    "w = np.random.random_sample(d)\n",
    "b = np.random.random_sample(1)\n",
    "grad_descent_MSE(w, b, trainData, trainTarget, 0.1, 5, 0.1, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tuning the Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to plot loss and accuracy\n",
    "def plot_loss(x, train_loss=None, valid_loss=None, test_loss=None, title=None):\n",
    "    if train_loss != None:\n",
    "        plt.plot(x, train_loss, label=\"Training Loss\")\n",
    "    if valid_loss != None:\n",
    "        plt.plot(x, valid_loss, label=\"Validation Loss\")\n",
    "    if test_loss != None:\n",
    "        plt.plot(x, test_loss, label=\"Testing Loss\")\n",
    "    \n",
    "    if title == None:\n",
    "        plt.title(\"Training Loss\")\n",
    "    else:\n",
    "        plt.title(title)\n",
    "    \n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.xlim(left=0)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def plot_accuracy(x, train_accuracy=None, valid_accuracy=None, test_accuracy=None, title=None):\n",
    "    if train_accuracy != None:\n",
    "        plt.plot(x, train_accuracy, label=\"Training Accuracy\")\n",
    "    if valid_accuracy != None:\n",
    "        plt.plot(x, valid_accuracy, label=\"Validation Accuracy\")\n",
    "    if test_accuracy != None:\n",
    "        plt.plot(x, test_accuracy, label=\"Testing Accuracy\")\n",
    "    \n",
    "    if title == None:\n",
    "        plt.title(\"Accuracy\")\n",
    "    else:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.xlim(left=0)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "    plt.grid(linestyle='-', axis='y')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation of Gradient Descent with 5000 epochs and \\lambda = 0. Investigate the\n",
    "# impact of learning rate, \\alpha = 0.005, 0.001, 0.0001 on the performance of your classifier. \n",
    "# Plot the training, validation and test losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate impact by modifying the regularization parameter, \\lambda = {0.001, 0.1, 0.5}. \n",
    "# Plot the training, validation and test loss for \\alpha = 0:005 and report the final training, \n",
    "# validation and test accuracy of your classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Comparing Batch GD with normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(X, y):\n",
    "    N = X.shape[0]\n",
    "    d = X.shape[1] * X.shape[2]\n",
    "    if N < d:\n",
    "        return X.T.dot(np.linalg.inv( np.dot(X, X.T) )).dot(y)\n",
    "    else:\n",
    "        return np.linalg.inv( np.dot(X.T, X) ).dot(X.T).dot(y)\n",
    "        \n",
    "# compare above to gradient descent solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Logistic Regression\n",
    "## 2.1 Binary cross-entropy loss\n",
    "### 1. Loss Function and Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will work for both scalar and vector z\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "# Cross Entropy Loss\n",
    "def crossEntropyLoss(W, b, x, y, reg):\n",
    "    \n",
    "    X, W = augment(X, W, b)\n",
    "    \n",
    "    y_hat = sigmoid(x.dot(W) + b)\n",
    "    L = np.vectorize(lambda x,y: -np.log(x) if y == 1 else -np.log(1-x))(y_hat, y)\n",
    "    #... not finished\n",
    "\n",
    "def gradCE(W, b, x, y, reg):\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(W, b, x, y, alpha, epochs, reg, error_tol, lossType=\"MSE\"):\n",
    "    if lossType == \"MSE\":\n",
    "        return grad_descent_MSE(W, b, x, y, alpha, epochs, reg, error_tol)\n",
    "    elif lossType == \"CE\":\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Variable 'lossType' must be either 'MSE' or 'CE'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Comparision to Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For zero weight decay, learning rate of 0.005 and 5000 epochs, \n",
    "# plot the training cross entropy loss and MSE loss for \n",
    "# logistic regression and linear regression respectively.\n",
    "# Comment on the effect of cross-entropy loss convergence behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Batch Gradient Descent vs. SGD and Adam\n",
    "## 3.1 SGD\n",
    "### 1. Building the Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildGraph(loss=\"MSE\"):\n",
    "    #Initialize weight and bias tensors\n",
    "    tf.set_random_seed(421)\n",
    "\n",
    "    if loss == \"MSE\":\n",
    "        # Your implementation\n",
    "        pass\n",
    "    elif loss == \"CE\":\n",
    "        #Your implementation here\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Variable 'lossType' must be either 'MSE' or 'CE'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementing Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the SGD algorithm for a minibatch size of 500 \n",
    "# optimizing over 700 epochs 2, minimizing the MSE (you will repeat this for the CE later).\n",
    "# Calculate the total number of batches required by dividing the number\n",
    "# of training instances by the minibatch size. After each epoch you will need to reshuffle the\n",
    "# training data and start sampling from the beginning again. Initially, set \\lambda = 0 and continue\n",
    "# to use the same \\alpha value (i.e. 0.001). After each epoch, store the training, validation and test\n",
    "# losses and accuracies. Use these to plot the loss and accuracy curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Batch Size Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hyperparameter Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Cross Entropy Loss Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Comparison against Batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
