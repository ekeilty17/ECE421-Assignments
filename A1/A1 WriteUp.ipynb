{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given by the assignment\n",
    "def loadData():\n",
    "    with np.load('notMNIST.npz') as data :\n",
    "        Data, Target = data['images'], data['labels']\n",
    "        posClass = 2\n",
    "        negClass = 9\n",
    "        dataIndx = (Target==posClass) + (Target==negClass)\n",
    "        Data = Data[dataIndx]/255.\n",
    "        Target = Target[dataIndx].reshape(-1, 1)\n",
    "        Target[Target==posClass] = 1\n",
    "        Target[Target==negClass] = 0\n",
    "        np.random.seed(421)\n",
    "        randIndx = np.arange(len(Data))\n",
    "        np.random.shuffle(randIndx)\n",
    "        Data, Target = Data[randIndx], Target[randIndx]\n",
    "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "        testData, testTarget = Data[3600:], Target[3600:]\n",
    "    return trainData, validData, testData, trainTarget, validTarget, testTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: (3500, 28, 28)\tTraining tagets: (3500, 1)\n",
      "Validation Data: (100, 28, 28)\tValidation tagets: (100, 1)\n",
      "Testing Data: (145, 28, 28)\tTesting tagets:(145, 1)\n"
     ]
    }
   ],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "print(f\"Training Data: {trainData.shape}\\tTraining tagets: {trainTarget.shape}\")\n",
    "print(f\"Validation Data: {validData.shape}\\tValidation tagets: {validTarget.shape}\")\n",
    "print(f\"Testing Data: {testData.shape}\\tTesting tagets:{testTarget.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(image, target):\n",
    "    plt.imshow(image, cmap=\"hot\")\n",
    "    plt.title('J' if target == 0 else 'C')\n",
    "    # targets are binary encoded 0 == 'J' and 1 == 'C'\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(trainData[0], trainTarget[0])\n",
    "plot(trainData[1], trainTarget[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(X, w, b):\n",
    "    # flatten X\n",
    "    if len(X.shape) == 3:\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "    \n",
    "    # insert 1's at position 0 along the columns\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    # insert b at the front of W\n",
    "    w = np.insert(w, 0, b, axis=0)\n",
    "    \n",
    "    return X, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    return X.dot(w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(w, b, X, y):\n",
    "    y = y.reshape(-1)\n",
    "    y_pred = predict(w, b, X)\n",
    "    y_pred = np.vectorize(lambda z: 1 if z > 0 else 0)(y_pred)\n",
    "    return sum(y_pred == y) / y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Linear Regression\n",
    "### 1. Loss Function and Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.metrics import mean_squared_error\\n\\nX = trainData\\ny = trainTarget\\nN = X.shape[0]\\nd = X.shape[1] * X.shape[2]\\n\\nw = np.random.random_sample(d)\\nb = np.random.random_sample(1)\\n\\ny_pred = predict(w_LS, b_LS, X)\\nprint(mean_squared_error(y, y_pred))\\nprint(MSE(w_LS, b_LS, X, y, 0))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Squared Error Loss\n",
    "def MSE(w, b, X, y, reg):\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    y = y.reshape(-1)\n",
    "    return np.square(X.dot(w) + b - y).mean() + reg * np.square(w).sum()\n",
    "\n",
    "def gradMSE(w, b, X, y, reg):\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    y = y.reshape(-1)\n",
    "    N = y.shape[0]\n",
    "    \n",
    "    w_grad = 2.0/N * X.T.dot(X.dot(w) + b - y) + reg * w\n",
    "    b_grad = 2.0/N * np.sum(X.dot(w) + b - y)\n",
    "    return w_grad, b_grad\n",
    "\n",
    "# The below is a test for MSE Loss, which is correct (at least without the regulator)\n",
    "\"\"\"\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = trainData\n",
    "y = trainTarget\n",
    "N = X.shape[0]\n",
    "d = X.shape[1] * X.shape[2]\n",
    "\n",
    "w = np.random.random_sample(d)\n",
    "b = np.random.random_sample(1)\n",
    "\n",
    "y_pred = predict(w_LS, b_LS, X)\n",
    "print(mean_squared_error(y, y_pred))\n",
    "print(MSE(w_LS, b_LS, X, y, 0))\n",
    "\"\"\"\n",
    "\n",
    "#gradMSE(w, b, X, y, 0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3277.3605\tTraining acc: 49.80%\n",
      "Training loss: 544.8869\tTraining acc: 49.60%\n",
      "Training loss: 149.1921\tTraining acc: 50.11%\n",
      "Training loss: 84.8041\tTraining acc: 46.91%\n",
      "Training loss: 68.6349\tTraining acc: 51.94%\n",
      "Training loss: 60.3853\tTraining acc: 52.03%\n",
      "Training loss: 54.1175\tTraining acc: 52.97%\n",
      "Training loss: 48.8249\tTraining acc: 53.51%\n",
      "Training loss: 44.2450\tTraining acc: 53.94%\n",
      "Training loss: 40.2489\tTraining acc: 54.37%\n",
      "Training loss: 36.7453\tTraining acc: 54.74%\n",
      "Training loss: 33.6620\tTraining acc: 55.20%\n",
      "Training loss: 30.9392\tTraining acc: 55.51%\n",
      "Training loss: 28.5270\tTraining acc: 55.91%\n",
      "Training loss: 26.3834\tTraining acc: 56.20%\n",
      "Training loss: 24.4727\tTraining acc: 56.20%\n",
      "Training loss: 22.7645\tTraining acc: 56.11%\n",
      "Training loss: 21.2332\tTraining acc: 56.14%\n",
      "Training loss: 19.8563\tTraining acc: 56.37%\n",
      "Training loss: 18.6150\tTraining acc: 56.34%\n",
      "Training loss: 17.4928\tTraining acc: 56.66%\n",
      "Training loss: 16.4756\tTraining acc: 56.94%\n",
      "Training loss: 15.5512\tTraining acc: 56.97%\n",
      "Training loss: 14.7089\tTraining acc: 57.11%\n",
      "Training loss: 13.9396\tTraining acc: 56.91%\n",
      "Training loss: 13.2352\tTraining acc: 57.23%\n",
      "Training loss: 12.5888\tTraining acc: 57.26%\n",
      "Training loss: 11.9941\tTraining acc: 57.51%\n",
      "Training loss: 11.4459\tTraining acc: 57.74%\n",
      "Training loss: 10.9394\tTraining acc: 57.60%\n",
      "Training loss: 10.4706\tTraining acc: 57.71%\n",
      "Training loss: 10.0356\tTraining acc: 57.60%\n",
      "Training loss: 9.6314\tTraining acc: 57.74%\n",
      "Training loss: 9.2550\tTraining acc: 58.11%\n",
      "Training loss: 8.9040\tTraining acc: 58.14%\n",
      "Training loss: 8.5760\tTraining acc: 58.40%\n",
      "Training loss: 8.2690\tTraining acc: 58.29%\n",
      "Training loss: 7.9813\tTraining acc: 58.11%\n",
      "Training loss: 7.7112\tTraining acc: 58.17%\n",
      "Training loss: 7.4573\tTraining acc: 58.20%\n",
      "Training loss: 7.2183\tTraining acc: 58.23%\n",
      "Training loss: 6.9931\tTraining acc: 58.14%\n",
      "Training loss: 6.7805\tTraining acc: 58.57%\n",
      "Training loss: 6.5797\tTraining acc: 58.69%\n",
      "Training loss: 6.3897\tTraining acc: 58.86%\n",
      "Training loss: 6.2097\tTraining acc: 58.91%\n",
      "Training loss: 6.0392\tTraining acc: 58.74%\n",
      "Training loss: 5.8773\tTraining acc: 58.69%\n",
      "Training loss: 5.7236\tTraining acc: 58.71%\n",
      "Training loss: 5.5774\tTraining acc: 58.83%\n",
      "Training loss: 5.4383\tTraining acc: 59.03%\n",
      "Training loss: 5.3058\tTraining acc: 58.97%\n",
      "Training loss: 5.1795\tTraining acc: 58.89%\n",
      "Training loss: 5.0590\tTraining acc: 59.00%\n",
      "Training loss: 4.9439\tTraining acc: 59.20%\n",
      "Training loss: 4.8340\tTraining acc: 59.31%\n",
      "Training loss: 4.7289\tTraining acc: 59.31%\n",
      "Training loss: 4.6283\tTraining acc: 59.37%\n",
      "Training loss: 4.5319\tTraining acc: 59.54%\n",
      "Training loss: 4.4396\tTraining acc: 59.54%\n",
      "Training loss: 4.3511\tTraining acc: 59.63%\n",
      "Training loss: 4.2661\tTraining acc: 59.83%\n",
      "Training loss: 4.1846\tTraining acc: 59.89%\n",
      "Training loss: 4.1062\tTraining acc: 59.89%\n",
      "Training loss: 4.0309\tTraining acc: 60.06%\n",
      "Training loss: 3.9585\tTraining acc: 60.11%\n",
      "Training loss: 3.8887\tTraining acc: 60.26%\n",
      "Training loss: 3.8216\tTraining acc: 60.43%\n",
      "Training loss: 3.7569\tTraining acc: 60.51%\n",
      "Training loss: 3.6945\tTraining acc: 60.51%\n",
      "Training loss: 3.6344\tTraining acc: 60.57%\n",
      "Training loss: 3.5763\tTraining acc: 60.77%\n",
      "Training loss: 3.5202\tTraining acc: 60.80%\n",
      "Training loss: 3.4661\tTraining acc: 60.80%\n",
      "Training loss: 3.4137\tTraining acc: 60.89%\n",
      "Training loss: 3.3631\tTraining acc: 61.00%\n",
      "Training loss: 3.3142\tTraining acc: 60.94%\n",
      "Training loss: 3.2668\tTraining acc: 60.91%\n",
      "Training loss: 3.2209\tTraining acc: 60.91%\n",
      "Training loss: 3.1764\tTraining acc: 60.97%\n",
      "Training loss: 3.1333\tTraining acc: 60.97%\n",
      "Training loss: 3.0915\tTraining acc: 61.03%\n",
      "Training loss: 3.0510\tTraining acc: 61.14%\n",
      "Training loss: 3.0117\tTraining acc: 61.03%\n",
      "Training loss: 2.9735\tTraining acc: 61.09%\n",
      "Training loss: 2.9364\tTraining acc: 61.03%\n",
      "Training loss: 2.9003\tTraining acc: 61.06%\n",
      "Training loss: 2.8653\tTraining acc: 61.06%\n",
      "Training loss: 2.8312\tTraining acc: 61.03%\n",
      "Training loss: 2.7981\tTraining acc: 61.23%\n",
      "Training loss: 2.7658\tTraining acc: 61.37%\n",
      "Training loss: 2.7344\tTraining acc: 61.43%\n",
      "Training loss: 2.7039\tTraining acc: 61.40%\n",
      "Training loss: 2.6741\tTraining acc: 61.46%\n",
      "Training loss: 2.6451\tTraining acc: 61.60%\n",
      "Training loss: 2.6168\tTraining acc: 61.57%\n",
      "Training loss: 2.5892\tTraining acc: 61.71%\n",
      "Training loss: 2.5623\tTraining acc: 61.71%\n",
      "Training loss: 2.5360\tTraining acc: 61.69%\n",
      "Training loss: 2.5104\tTraining acc: 61.83%\n",
      "Training loss: 2.4853\tTraining acc: 61.86%\n",
      "Training loss: 2.4609\tTraining acc: 62.03%\n",
      "Training loss: 2.4370\tTraining acc: 62.09%\n",
      "Training loss: 2.4137\tTraining acc: 62.11%\n",
      "Training loss: 2.3909\tTraining acc: 62.03%\n",
      "Training loss: 2.3686\tTraining acc: 61.94%\n",
      "Training loss: 2.3467\tTraining acc: 62.11%\n",
      "Training loss: 2.3254\tTraining acc: 62.06%\n",
      "Training loss: 2.3045\tTraining acc: 62.09%\n",
      "Training loss: 2.2841\tTraining acc: 62.11%\n",
      "Training loss: 2.2640\tTraining acc: 62.17%\n",
      "Training loss: 2.2444\tTraining acc: 62.26%\n",
      "Training loss: 2.2252\tTraining acc: 62.34%\n",
      "Training loss: 2.2064\tTraining acc: 62.31%\n",
      "Training loss: 2.1880\tTraining acc: 62.34%\n",
      "Training loss: 2.1699\tTraining acc: 62.34%\n",
      "Training loss: 2.1522\tTraining acc: 62.46%\n",
      "Training loss: 2.1348\tTraining acc: 62.43%\n",
      "Training loss: 2.1177\tTraining acc: 62.46%\n",
      "Training loss: 2.1010\tTraining acc: 62.37%\n",
      "Training loss: 2.0846\tTraining acc: 62.40%\n",
      "Training loss: 2.0685\tTraining acc: 62.40%\n",
      "Training loss: 2.0527\tTraining acc: 62.46%\n",
      "Training loss: 2.0371\tTraining acc: 62.49%\n",
      "Training loss: 2.0219\tTraining acc: 62.49%\n",
      "Training loss: 2.0069\tTraining acc: 62.57%\n",
      "Training loss: 1.9922\tTraining acc: 62.60%\n",
      "Training loss: 1.9777\tTraining acc: 62.66%\n",
      "Training loss: 1.9635\tTraining acc: 62.69%\n",
      "Training loss: 1.9495\tTraining acc: 62.71%\n",
      "Training loss: 1.9358\tTraining acc: 62.80%\n",
      "Training loss: 1.9223\tTraining acc: 62.80%\n",
      "Training loss: 1.9090\tTraining acc: 62.86%\n",
      "Training loss: 1.8959\tTraining acc: 62.86%\n",
      "Training loss: 1.8831\tTraining acc: 63.00%\n",
      "Training loss: 1.8704\tTraining acc: 63.03%\n",
      "Training loss: 1.8580\tTraining acc: 63.03%\n",
      "Training loss: 1.8458\tTraining acc: 63.17%\n",
      "Training loss: 1.8337\tTraining acc: 63.26%\n",
      "Training loss: 1.8218\tTraining acc: 63.26%\n",
      "Training loss: 1.8102\tTraining acc: 63.34%\n",
      "Training loss: 1.7986\tTraining acc: 63.37%\n",
      "Training loss: 1.7873\tTraining acc: 63.40%\n",
      "Training loss: 1.7761\tTraining acc: 63.40%\n",
      "Training loss: 1.7651\tTraining acc: 63.49%\n",
      "Training loss: 1.7543\tTraining acc: 63.57%\n",
      "Training loss: 1.7436\tTraining acc: 63.63%\n",
      "Training loss: 1.7331\tTraining acc: 63.80%\n",
      "Training loss: 1.7227\tTraining acc: 63.83%\n",
      "Training loss: 1.7125\tTraining acc: 63.77%\n",
      "Training loss: 1.7024\tTraining acc: 63.83%\n",
      "Training loss: 1.6925\tTraining acc: 63.86%\n",
      "Training loss: 1.6827\tTraining acc: 63.83%\n",
      "Training loss: 1.6730\tTraining acc: 63.83%\n",
      "Training loss: 1.6635\tTraining acc: 63.86%\n",
      "Training loss: 1.6541\tTraining acc: 63.83%\n",
      "Training loss: 1.6448\tTraining acc: 63.89%\n",
      "Training loss: 1.6356\tTraining acc: 63.86%\n",
      "Training loss: 1.6266\tTraining acc: 63.91%\n",
      "Training loss: 1.6177\tTraining acc: 63.97%\n",
      "Training loss: 1.6089\tTraining acc: 64.00%\n",
      "Training loss: 1.6002\tTraining acc: 63.94%\n",
      "Training loss: 1.5916\tTraining acc: 64.03%\n",
      "Training loss: 1.5832\tTraining acc: 64.09%\n",
      "Training loss: 1.5748\tTraining acc: 64.06%\n",
      "Training loss: 1.5666\tTraining acc: 64.09%\n",
      "Training loss: 1.5584\tTraining acc: 64.09%\n",
      "Training loss: 1.5504\tTraining acc: 64.06%\n",
      "Training loss: 1.5424\tTraining acc: 64.03%\n",
      "Training loss: 1.5346\tTraining acc: 63.97%\n",
      "Training loss: 1.5268\tTraining acc: 64.00%\n",
      "Training loss: 1.5191\tTraining acc: 64.03%\n",
      "Training loss: 1.5116\tTraining acc: 64.00%\n",
      "Training loss: 1.5041\tTraining acc: 64.03%\n",
      "Training loss: 1.4967\tTraining acc: 64.00%\n",
      "Training loss: 1.4894\tTraining acc: 63.97%\n",
      "Training loss: 1.4821\tTraining acc: 64.09%\n",
      "Training loss: 1.4750\tTraining acc: 64.17%\n",
      "Training loss: 1.4679\tTraining acc: 64.11%\n",
      "Training loss: 1.4610\tTraining acc: 64.17%\n",
      "Training loss: 1.4541\tTraining acc: 64.23%\n",
      "Training loss: 1.4472\tTraining acc: 64.31%\n",
      "Training loss: 1.4405\tTraining acc: 64.34%\n",
      "Training loss: 1.4338\tTraining acc: 64.34%\n",
      "Training loss: 1.4272\tTraining acc: 64.31%\n",
      "Training loss: 1.4207\tTraining acc: 64.23%\n",
      "Training loss: 1.4143\tTraining acc: 64.20%\n",
      "Training loss: 1.4079\tTraining acc: 64.20%\n",
      "Training loss: 1.4016\tTraining acc: 64.20%\n",
      "Training loss: 1.3953\tTraining acc: 64.23%\n",
      "Training loss: 1.3891\tTraining acc: 64.29%\n",
      "Training loss: 1.3830\tTraining acc: 64.29%\n",
      "Training loss: 1.3770\tTraining acc: 64.26%\n",
      "Training loss: 1.3710\tTraining acc: 64.31%\n",
      "Training loss: 1.3650\tTraining acc: 64.31%\n",
      "Training loss: 1.3592\tTraining acc: 64.29%\n",
      "Training loss: 1.3534\tTraining acc: 64.31%\n",
      "Training loss: 1.3476\tTraining acc: 64.34%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.3419\tTraining acc: 64.43%\n",
      "Training loss: 1.3363\tTraining acc: 64.49%\n",
      "Training loss: 1.3307\tTraining acc: 64.51%\n",
      "Training loss: 1.3252\tTraining acc: 64.51%\n",
      "Training loss: 1.3197\tTraining acc: 64.60%\n",
      "Training loss: 1.3143\tTraining acc: 64.51%\n",
      "Training loss: 1.3090\tTraining acc: 64.49%\n",
      "Training loss: 1.3036\tTraining acc: 64.46%\n",
      "Training loss: 1.2984\tTraining acc: 64.54%\n",
      "Training loss: 1.2932\tTraining acc: 64.49%\n",
      "Training loss: 1.2880\tTraining acc: 64.57%\n",
      "Training loss: 1.2829\tTraining acc: 64.60%\n",
      "Training loss: 1.2778\tTraining acc: 64.66%\n",
      "Training loss: 1.2728\tTraining acc: 64.66%\n",
      "Training loss: 1.2679\tTraining acc: 64.77%\n",
      "Training loss: 1.2629\tTraining acc: 64.77%\n",
      "Training loss: 1.2581\tTraining acc: 64.74%\n",
      "Training loss: 1.2532\tTraining acc: 64.77%\n",
      "Training loss: 1.2484\tTraining acc: 64.83%\n",
      "Training loss: 1.2437\tTraining acc: 64.80%\n",
      "Training loss: 1.2390\tTraining acc: 64.83%\n",
      "Training loss: 1.2343\tTraining acc: 64.80%\n",
      "Training loss: 1.2297\tTraining acc: 64.83%\n",
      "Training loss: 1.2251\tTraining acc: 64.80%\n",
      "Training loss: 1.2206\tTraining acc: 64.80%\n",
      "Training loss: 1.2161\tTraining acc: 64.80%\n",
      "Training loss: 1.2117\tTraining acc: 64.80%\n",
      "Training loss: 1.2072\tTraining acc: 64.83%\n",
      "Training loss: 1.2029\tTraining acc: 64.91%\n",
      "Training loss: 1.1985\tTraining acc: 64.94%\n",
      "Training loss: 1.1942\tTraining acc: 65.03%\n",
      "Training loss: 1.1899\tTraining acc: 65.06%\n",
      "Training loss: 1.1857\tTraining acc: 65.06%\n",
      "Training loss: 1.1815\tTraining acc: 65.03%\n",
      "Training loss: 1.1773\tTraining acc: 65.00%\n",
      "Training loss: 1.1732\tTraining acc: 65.03%\n",
      "Training loss: 1.1691\tTraining acc: 64.97%\n",
      "Training loss: 1.1651\tTraining acc: 64.97%\n",
      "Training loss: 1.1610\tTraining acc: 64.94%\n",
      "Training loss: 1.1570\tTraining acc: 64.94%\n",
      "Training loss: 1.1531\tTraining acc: 64.97%\n",
      "Training loss: 1.1491\tTraining acc: 64.97%\n",
      "Training loss: 1.1452\tTraining acc: 64.97%\n",
      "Training loss: 1.1414\tTraining acc: 64.97%\n",
      "Training loss: 1.1375\tTraining acc: 64.89%\n",
      "Training loss: 1.1337\tTraining acc: 64.91%\n",
      "Training loss: 1.1300\tTraining acc: 64.91%\n",
      "Training loss: 1.1262\tTraining acc: 64.89%\n",
      "Training loss: 1.1225\tTraining acc: 64.89%\n",
      "Training loss: 1.1188\tTraining acc: 64.91%\n",
      "Training loss: 1.1152\tTraining acc: 64.97%\n",
      "Training loss: 1.1115\tTraining acc: 64.97%\n",
      "Training loss: 1.1079\tTraining acc: 65.00%\n",
      "Training loss: 1.1043\tTraining acc: 65.03%\n",
      "Training loss: 1.1008\tTraining acc: 64.97%\n",
      "Training loss: 1.0973\tTraining acc: 64.97%\n",
      "Training loss: 1.0938\tTraining acc: 64.91%\n",
      "Training loss: 1.0903\tTraining acc: 65.06%\n",
      "Training loss: 1.0869\tTraining acc: 65.11%\n",
      "Training loss: 1.0834\tTraining acc: 65.06%\n",
      "Training loss: 1.0801\tTraining acc: 65.06%\n",
      "Training loss: 1.0767\tTraining acc: 65.09%\n",
      "Training loss: 1.0733\tTraining acc: 65.09%\n",
      "Training loss: 1.0700\tTraining acc: 65.14%\n",
      "Training loss: 1.0667\tTraining acc: 65.09%\n",
      "Training loss: 1.0635\tTraining acc: 65.06%\n",
      "Training loss: 1.0602\tTraining acc: 65.06%\n",
      "Training loss: 1.0570\tTraining acc: 65.00%\n",
      "Training loss: 1.0538\tTraining acc: 65.00%\n",
      "Training loss: 1.0506\tTraining acc: 64.94%\n",
      "Training loss: 1.0475\tTraining acc: 65.00%\n",
      "Training loss: 1.0443\tTraining acc: 65.03%\n",
      "Training loss: 1.0412\tTraining acc: 65.09%\n",
      "Training loss: 1.0381\tTraining acc: 65.09%\n",
      "Training loss: 1.0351\tTraining acc: 65.14%\n",
      "Training loss: 1.0320\tTraining acc: 65.14%\n",
      "Training loss: 1.0290\tTraining acc: 65.20%\n",
      "Training loss: 1.0260\tTraining acc: 65.23%\n",
      "Training loss: 1.0230\tTraining acc: 65.26%\n",
      "Training loss: 1.0200\tTraining acc: 65.26%\n",
      "Training loss: 1.0171\tTraining acc: 65.31%\n",
      "Training loss: 1.0142\tTraining acc: 65.29%\n",
      "Training loss: 1.0113\tTraining acc: 65.31%\n",
      "Training loss: 1.0084\tTraining acc: 65.31%\n",
      "Training loss: 1.0055\tTraining acc: 65.37%\n",
      "Training loss: 1.0027\tTraining acc: 65.34%\n",
      "Training loss: 0.9998\tTraining acc: 65.34%\n",
      "Training loss: 0.9970\tTraining acc: 65.31%\n",
      "Training loss: 0.9942\tTraining acc: 65.29%\n",
      "Training loss: 0.9915\tTraining acc: 65.31%\n",
      "Training loss: 0.9887\tTraining acc: 65.34%\n",
      "Training loss: 0.9860\tTraining acc: 65.37%\n",
      "Training loss: 0.9833\tTraining acc: 65.46%\n",
      "Training loss: 0.9806\tTraining acc: 65.49%\n",
      "Training loss: 0.9779\tTraining acc: 65.46%\n",
      "Training loss: 0.9752\tTraining acc: 65.49%\n",
      "Training loss: 0.9726\tTraining acc: 65.49%\n",
      "Training loss: 0.9699\tTraining acc: 65.49%\n",
      "Training loss: 0.9673\tTraining acc: 65.51%\n",
      "Training loss: 0.9647\tTraining acc: 65.51%\n",
      "Training loss: 0.9621\tTraining acc: 65.54%\n",
      "Training loss: 0.9596\tTraining acc: 65.54%\n",
      "Training loss: 0.9570\tTraining acc: 65.51%\n",
      "Training loss: 0.9545\tTraining acc: 65.54%\n",
      "Training loss: 0.9520\tTraining acc: 65.60%\n",
      "Training loss: 0.9495\tTraining acc: 65.63%\n",
      "Training loss: 0.9470\tTraining acc: 65.66%\n",
      "Training loss: 0.9445\tTraining acc: 65.66%\n",
      "Training loss: 0.9420\tTraining acc: 65.69%\n",
      "Training loss: 0.9396\tTraining acc: 65.66%\n",
      "Training loss: 0.9372\tTraining acc: 65.69%\n",
      "Training loss: 0.9348\tTraining acc: 65.63%\n",
      "Training loss: 0.9324\tTraining acc: 65.69%\n",
      "Training loss: 0.9300\tTraining acc: 65.74%\n",
      "Training loss: 0.9276\tTraining acc: 65.83%\n",
      "Training loss: 0.9252\tTraining acc: 65.83%\n",
      "Training loss: 0.9229\tTraining acc: 65.83%\n",
      "Training loss: 0.9206\tTraining acc: 65.77%\n",
      "Training loss: 0.9183\tTraining acc: 65.80%\n",
      "Training loss: 0.9160\tTraining acc: 65.86%\n",
      "Training loss: 0.9137\tTraining acc: 65.86%\n",
      "Training loss: 0.9114\tTraining acc: 65.86%\n",
      "Training loss: 0.9091\tTraining acc: 65.86%\n",
      "Training loss: 0.9069\tTraining acc: 65.86%\n",
      "Training loss: 0.9047\tTraining acc: 65.86%\n",
      "Training loss: 0.9024\tTraining acc: 65.89%\n",
      "Training loss: 0.9002\tTraining acc: 65.89%\n",
      "Training loss: 0.8980\tTraining acc: 65.91%\n",
      "Training loss: 0.8959\tTraining acc: 65.94%\n",
      "Training loss: 0.8937\tTraining acc: 65.97%\n",
      "Training loss: 0.8915\tTraining acc: 66.00%\n",
      "Training loss: 0.8894\tTraining acc: 66.03%\n",
      "Training loss: 0.8873\tTraining acc: 66.00%\n",
      "Training loss: 0.8851\tTraining acc: 66.00%\n",
      "Training loss: 0.8830\tTraining acc: 66.03%\n",
      "Training loss: 0.8809\tTraining acc: 66.03%\n",
      "Training loss: 0.8788\tTraining acc: 66.03%\n",
      "Training loss: 0.8768\tTraining acc: 66.00%\n",
      "Training loss: 0.8747\tTraining acc: 66.00%\n",
      "Training loss: 0.8727\tTraining acc: 65.97%\n",
      "Training loss: 0.8706\tTraining acc: 65.94%\n",
      "Training loss: 0.8686\tTraining acc: 66.00%\n",
      "Training loss: 0.8666\tTraining acc: 66.00%\n",
      "Training loss: 0.8646\tTraining acc: 66.00%\n",
      "Training loss: 0.8626\tTraining acc: 66.03%\n",
      "Training loss: 0.8606\tTraining acc: 66.06%\n",
      "Training loss: 0.8586\tTraining acc: 66.03%\n",
      "Training loss: 0.8566\tTraining acc: 66.06%\n",
      "Training loss: 0.8547\tTraining acc: 66.11%\n",
      "Training loss: 0.8527\tTraining acc: 66.06%\n",
      "Training loss: 0.8508\tTraining acc: 66.06%\n",
      "Training loss: 0.8489\tTraining acc: 66.14%\n",
      "Training loss: 0.8470\tTraining acc: 66.17%\n",
      "Training loss: 0.8451\tTraining acc: 66.20%\n",
      "Training loss: 0.8432\tTraining acc: 66.17%\n",
      "Training loss: 0.8413\tTraining acc: 66.17%\n",
      "Training loss: 0.8394\tTraining acc: 66.17%\n",
      "Training loss: 0.8376\tTraining acc: 66.14%\n",
      "Training loss: 0.8357\tTraining acc: 66.17%\n",
      "Training loss: 0.8339\tTraining acc: 66.20%\n",
      "Training loss: 0.8320\tTraining acc: 66.23%\n",
      "Training loss: 0.8302\tTraining acc: 66.23%\n",
      "Training loss: 0.8284\tTraining acc: 66.23%\n",
      "Training loss: 0.8266\tTraining acc: 66.23%\n",
      "Training loss: 0.8248\tTraining acc: 66.20%\n",
      "Training loss: 0.8230\tTraining acc: 66.20%\n",
      "Training loss: 0.8212\tTraining acc: 66.26%\n",
      "Training loss: 0.8195\tTraining acc: 66.29%\n",
      "Training loss: 0.8177\tTraining acc: 66.29%\n",
      "Training loss: 0.8160\tTraining acc: 66.29%\n",
      "Training loss: 0.8142\tTraining acc: 66.29%\n",
      "Training loss: 0.8125\tTraining acc: 66.31%\n",
      "Training loss: 0.8108\tTraining acc: 66.29%\n",
      "Training loss: 0.8091\tTraining acc: 66.29%\n",
      "Training loss: 0.8074\tTraining acc: 66.31%\n",
      "Training loss: 0.8057\tTraining acc: 66.31%\n",
      "Training loss: 0.8040\tTraining acc: 66.37%\n",
      "Training loss: 0.8023\tTraining acc: 66.46%\n",
      "Training loss: 0.8006\tTraining acc: 66.46%\n",
      "Training loss: 0.7989\tTraining acc: 66.46%\n",
      "Training loss: 0.7973\tTraining acc: 66.51%\n",
      "Training loss: 0.7956\tTraining acc: 66.54%\n",
      "Training loss: 0.7940\tTraining acc: 66.54%\n",
      "Training loss: 0.7924\tTraining acc: 66.60%\n",
      "Training loss: 0.7907\tTraining acc: 66.60%\n",
      "Training loss: 0.7891\tTraining acc: 66.63%\n",
      "Training loss: 0.7875\tTraining acc: 66.69%\n",
      "Training loss: 0.7859\tTraining acc: 66.71%\n",
      "Training loss: 0.7843\tTraining acc: 66.77%\n",
      "Training loss: 0.7827\tTraining acc: 66.77%\n",
      "Training loss: 0.7812\tTraining acc: 66.77%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7796\tTraining acc: 66.77%\n",
      "Training loss: 0.7780\tTraining acc: 66.83%\n",
      "Training loss: 0.7765\tTraining acc: 66.80%\n",
      "Training loss: 0.7749\tTraining acc: 66.83%\n",
      "Training loss: 0.7734\tTraining acc: 66.86%\n",
      "Training loss: 0.7719\tTraining acc: 66.86%\n",
      "Training loss: 0.7703\tTraining acc: 66.86%\n",
      "Training loss: 0.7688\tTraining acc: 66.91%\n",
      "Training loss: 0.7673\tTraining acc: 66.97%\n",
      "Training loss: 0.7658\tTraining acc: 67.03%\n",
      "Training loss: 0.7643\tTraining acc: 67.03%\n",
      "Training loss: 0.7628\tTraining acc: 67.03%\n",
      "Training loss: 0.7613\tTraining acc: 67.06%\n",
      "Training loss: 0.7599\tTraining acc: 67.14%\n",
      "Training loss: 0.7584\tTraining acc: 67.11%\n",
      "Training loss: 0.7569\tTraining acc: 67.09%\n",
      "Training loss: 0.7555\tTraining acc: 67.11%\n",
      "Training loss: 0.7540\tTraining acc: 67.14%\n",
      "Training loss: 0.7526\tTraining acc: 67.14%\n",
      "Training loss: 0.7511\tTraining acc: 67.14%\n",
      "Training loss: 0.7497\tTraining acc: 67.20%\n",
      "Training loss: 0.7483\tTraining acc: 67.23%\n",
      "Training loss: 0.7469\tTraining acc: 67.26%\n",
      "Training loss: 0.7455\tTraining acc: 67.26%\n",
      "Training loss: 0.7441\tTraining acc: 67.29%\n",
      "Training loss: 0.7427\tTraining acc: 67.29%\n",
      "Training loss: 0.7413\tTraining acc: 67.31%\n",
      "Training loss: 0.7399\tTraining acc: 67.31%\n",
      "Training loss: 0.7385\tTraining acc: 67.31%\n",
      "Training loss: 0.7371\tTraining acc: 67.34%\n",
      "Training loss: 0.7358\tTraining acc: 67.34%\n",
      "Training loss: 0.7344\tTraining acc: 67.34%\n",
      "Training loss: 0.7331\tTraining acc: 67.34%\n",
      "Training loss: 0.7317\tTraining acc: 67.40%\n",
      "Training loss: 0.7304\tTraining acc: 67.43%\n",
      "Training loss: 0.7290\tTraining acc: 67.43%\n",
      "Training loss: 0.7277\tTraining acc: 67.46%\n",
      "Training loss: 0.7264\tTraining acc: 67.46%\n",
      "Training loss: 0.7251\tTraining acc: 67.46%\n",
      "Training loss: 0.7237\tTraining acc: 67.46%\n",
      "Training loss: 0.7224\tTraining acc: 67.40%\n",
      "Training loss: 0.7211\tTraining acc: 67.37%\n",
      "Training loss: 0.7198\tTraining acc: 67.34%\n",
      "Training loss: 0.7185\tTraining acc: 67.34%\n",
      "Training loss: 0.7173\tTraining acc: 67.34%\n",
      "Training loss: 0.7160\tTraining acc: 67.34%\n",
      "Training loss: 0.7147\tTraining acc: 67.34%\n",
      "Training loss: 0.7134\tTraining acc: 67.40%\n",
      "Training loss: 0.7122\tTraining acc: 67.40%\n",
      "Training loss: 0.7109\tTraining acc: 67.40%\n",
      "Training loss: 0.7097\tTraining acc: 67.34%\n",
      "Training loss: 0.7084\tTraining acc: 67.34%\n",
      "Training loss: 0.7072\tTraining acc: 67.34%\n",
      "Training loss: 0.7059\tTraining acc: 67.40%\n",
      "Training loss: 0.7047\tTraining acc: 67.40%\n",
      "Training loss: 0.7035\tTraining acc: 67.40%\n",
      "Training loss: 0.7023\tTraining acc: 67.37%\n",
      "Training loss: 0.7010\tTraining acc: 67.37%\n",
      "Training loss: 0.6998\tTraining acc: 67.37%\n",
      "Training loss: 0.6986\tTraining acc: 67.34%\n",
      "Training loss: 0.6974\tTraining acc: 67.40%\n",
      "Training loss: 0.6962\tTraining acc: 67.34%\n",
      "Training loss: 0.6950\tTraining acc: 67.37%\n",
      "Training loss: 0.6939\tTraining acc: 67.37%\n",
      "Training loss: 0.6927\tTraining acc: 67.37%\n",
      "Training loss: 0.6915\tTraining acc: 67.37%\n",
      "Training loss: 0.6903\tTraining acc: 67.40%\n",
      "Training loss: 0.6892\tTraining acc: 67.43%\n",
      "Training loss: 0.6880\tTraining acc: 67.43%\n",
      "Training loss: 0.6869\tTraining acc: 67.43%\n",
      "Training loss: 0.6857\tTraining acc: 67.43%\n",
      "Training loss: 0.6846\tTraining acc: 67.43%\n",
      "Training loss: 0.6834\tTraining acc: 67.46%\n",
      "Training loss: 0.6823\tTraining acc: 67.43%\n",
      "Training loss: 0.6811\tTraining acc: 67.40%\n",
      "Training loss: 0.6800\tTraining acc: 67.40%\n",
      "Training loss: 0.6789\tTraining acc: 67.49%\n",
      "Training loss: 0.6778\tTraining acc: 67.49%\n",
      "Training loss: 0.6767\tTraining acc: 67.46%\n",
      "Training loss: 0.6755\tTraining acc: 67.46%\n",
      "Training loss: 0.6744\tTraining acc: 67.43%\n",
      "Training loss: 0.6733\tTraining acc: 67.43%\n",
      "Training loss: 0.6722\tTraining acc: 67.43%\n",
      "Training loss: 0.6711\tTraining acc: 67.46%\n",
      "Training loss: 0.6701\tTraining acc: 67.49%\n",
      "Training loss: 0.6690\tTraining acc: 67.49%\n",
      "Training loss: 0.6679\tTraining acc: 67.54%\n",
      "Training loss: 0.6668\tTraining acc: 67.57%\n",
      "Training loss: 0.6657\tTraining acc: 67.57%\n",
      "Training loss: 0.6647\tTraining acc: 67.57%\n",
      "Training loss: 0.6636\tTraining acc: 67.54%\n",
      "Training loss: 0.6626\tTraining acc: 67.54%\n",
      "Training loss: 0.6615\tTraining acc: 67.51%\n",
      "Training loss: 0.6604\tTraining acc: 67.49%\n",
      "Training loss: 0.6594\tTraining acc: 67.49%\n",
      "Training loss: 0.6584\tTraining acc: 67.51%\n",
      "Training loss: 0.6573\tTraining acc: 67.51%\n",
      "Training loss: 0.6563\tTraining acc: 67.54%\n",
      "Training loss: 0.6553\tTraining acc: 67.51%\n",
      "Training loss: 0.6542\tTraining acc: 67.51%\n",
      "Training loss: 0.6532\tTraining acc: 67.54%\n",
      "Training loss: 0.6522\tTraining acc: 67.54%\n",
      "Training loss: 0.6512\tTraining acc: 67.51%\n",
      "Training loss: 0.6502\tTraining acc: 67.57%\n",
      "Training loss: 0.6492\tTraining acc: 67.57%\n",
      "Training loss: 0.6481\tTraining acc: 67.57%\n",
      "Training loss: 0.6471\tTraining acc: 67.57%\n"
     ]
    }
   ],
   "source": [
    "def grad_descent_MSE(w, b, X, y, alpha, epochs, reg, error_tol, validData=None, validTarget=None, testData=None, testTarget=None):\n",
    "    train_loss, train_acc = [], []\n",
    "    valid_loss, valid_acc = [], []\n",
    "    test_loss, test_acc = [], []\n",
    "    printing = True\n",
    "    for i in range(epochs):\n",
    "        grad_w, grad_b = gradMSE(w, b, X, y, reg)\n",
    "        w -= alpha * grad_w\n",
    "        b -= alpha * grad_b\n",
    "        \n",
    "        # Calculating Statistics\n",
    "        train_loss.append( MSE(w, b, X, y, reg) )\n",
    "        train_acc.append( accuracy(w, b, X, y) )\n",
    "\n",
    "        if validData != None and validTarget != None:\n",
    "            valid_loss.append( MSE(w, b, validData, validTarget, reg) )\n",
    "            valid_acc.append( accuracy(w, b, validData, validTarget) )\n",
    "        if testData != None and testTarget != None:\n",
    "            test_loss.append( MSE(w, b, testData, testTarget, reg) )\n",
    "            valid_acc.append( accuracy(w, b, testData, testTarget) )\n",
    "        \n",
    "        # Print Losses and Accurancies if printing is on\n",
    "        if printing:\n",
    "            print(f\"Training loss: {train_loss[-1]:.4f}\\tTraining acc: {train_acc[-1]*100:.2f}%\")\n",
    "            if validData != None and validTarget != None:\n",
    "                print(f\"Validation loss: {valid_loss[-1]:.4f}\\tValidation acc: {valid_acc[-1]*100:.2f}%\")\n",
    "            if testData != None and testTarget != None:\n",
    "                print(f\"Testing loss: {test_loss[-1]:.4f}\\tTesting acc: {test_acc[-1]*100:.2f}%\")\n",
    "\n",
    "        # Check stopping condition\n",
    "        if i > 1 and np.abs(train_loss[-2] - train_loss[-1]) <= error_tol:\n",
    "            break\n",
    "\n",
    "    statistics = (train_loss, train_acc)\n",
    "    if validData != None and validTarget != None:\n",
    "        statistics += (valid_loss, valid_acc, )\n",
    "    if testData != None and testTarget != None:\n",
    "        statistics += (test_loss, test_acc,)\n",
    "    # Python 3.8 made this easier, but 3.7 you have to do this\n",
    "    out = (w, b, *statistics)\n",
    "    \n",
    "    return out\n",
    "\n",
    "X = trainData\n",
    "N = X.shape[0]\n",
    "d = X.shape[1] * X.shape[2]\n",
    "\n",
    "w = np.random.random_sample(d)\n",
    "b = np.random.random_sample(1)\n",
    "w, b, *statistics = grad_descent_MSE(w, b, trainData, trainTarget, 0.005, 5000, 0, 0.001)\n",
    "#train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = statistics\n",
    "train_loss, train_acc = statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tuning the Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplot_loss(np.arange(0, len(train_loss), 1), train_loss)#, valid_loss, test_loss)\\nplot_accuracy(np.arange(0, len(train_loss), 1), train_acc)#, valid_acc, test_acc)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# functions to plot loss and accuracy\n",
    "def plot_loss(x, train_loss=None, valid_loss=None, test_loss=None, title=None):\n",
    "    if train_loss != None:\n",
    "        plt.plot(x, train_loss, label=\"Training Loss\")\n",
    "    if valid_loss != None:\n",
    "        plt.plot(x, valid_loss, label=\"Validation Loss\")\n",
    "    if test_loss != None:\n",
    "        plt.plot(x, test_loss, label=\"Testing Loss\")\n",
    "    \n",
    "    if title == None:\n",
    "        plt.title(\"Training Loss\")\n",
    "    else:\n",
    "        plt.title(title)\n",
    "    \n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.xlim(left=0)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(x, train_accuracy=None, valid_accuracy=None, test_accuracy=None, title=None):\n",
    "    if train_accuracy != None:\n",
    "        plt.plot(x, train_accuracy, label=\"Training Accuracy\")\n",
    "    if valid_accuracy != None:\n",
    "        plt.plot(x, valid_accuracy, label=\"Validation Accuracy\")\n",
    "    if test_accuracy != None:\n",
    "        plt.plot(x, test_accuracy, label=\"Testing Accuracy\")\n",
    "    \n",
    "    if title == None:\n",
    "        plt.title(\"Accuracy\")\n",
    "    else:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.xlim(left=0)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "    plt.grid(linestyle='-', axis='y')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"\n",
    "plot_loss(np.arange(0, len(train_loss), 1), train_loss)#, valid_loss, test_loss)\n",
    "plot_accuracy(np.arange(0, len(train_loss), 1), train_acc)#, valid_acc, test_acc)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation of Gradient Descent with 5000 epochs and \\lambda = 0. Investigate the\n",
    "# impact of learning rate, \\alpha = 0.005, 0.001, 0.0001 on the performance of your classifier. \n",
    "# Plot the training, validation and test losses.\n",
    "\n",
    "# Eric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate impact by modifying the regularization parameter, \\lambda = {0.001, 0.1, 0.5}. \n",
    "# Plot the training, validation and test loss for \\alpha = 0:005 and report the final training, \n",
    "# validation and test accuracy of your classifier.\n",
    "\n",
    "# Sandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Comparing Batch GD with normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Squares Training loss: 0.0187\tLeast Squares Training acc: 71.29%\n",
      "Least Squares Validation loss: 0.0476\tLeast Squares Validation acc: 69.00%\n",
      "Least Squares Testing loss: 0.0570\tLeast Squares Testing acc: 66.90%\n"
     ]
    }
   ],
   "source": [
    "def least_squares(X, y):\n",
    "    N = X.shape[0]\n",
    "    d = X.shape[1] * X.shape[2]\n",
    "    X, _ = augment(X, np.zeros(X.shape[0]), 0)\n",
    "    y = y.reshape(-1)\n",
    "    if N < d:\n",
    "        w_aug = X.T.dot(np.linalg.inv( np.dot(X, X.T) )).dot(y)\n",
    "    else:\n",
    "        w_aug = np.linalg.inv( np.dot(X.T, X) ).dot(X.T).dot(y)\n",
    "    \n",
    "    return w_aug[1:], w_aug[0]\n",
    "        \n",
    "# compare above to gradient descent solution\n",
    "w_LS, b_LS = least_squares(trainData, trainTarget)\n",
    "\n",
    "loss = MSE(w_LS, b_LS, trainData, trainTarget, 0)\n",
    "acc = accuracy(w_LS, b_LS, trainData, trainTarget)\n",
    "print(f\"Least Squares Training loss: {loss:.4f}\\tLeast Squares Training acc: {100*acc:.2f}%\")\n",
    "loss = MSE(w_LS, b_LS, validData, validTarget, 0)\n",
    "acc = accuracy(w_LS, b_LS, validData, validTarget)\n",
    "print(f\"Least Squares Validation loss: {loss:.4f}\\tLeast Squares Validation acc: {100*acc:.2f}%\")\n",
    "loss = MSE(w_LS, b_LS, testData, testTarget, 0)\n",
    "acc = accuracy(w_LS, b_LS, testData, testTarget)\n",
    "print(f\"Least Squares Testing loss: {loss:.4f}\\tLeast Squares Testing acc: {100*acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the analytical solution, the training loss achieved with the analytical equation is 0.0187 with a training accuracy of 71.29%. The training loss and accuracies for batch gradient descent were respectively\n",
    "# INSERT VALUE HERE\n",
    "and \n",
    "# INSERT VALUE HERE\n",
    ". From the values, we see that the analytical solution performed better. However, computing it grows increasingly difficult with the size of the problem. As the problem scales, batch gradient descent allows for faster convergence with comparable accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Logistic Regression\n",
    "## 2.1 Binary cross-entropy loss\n",
    "### 1. Loss Function and Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will work for both scalar and vector z\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "# Cross Entropy Loss\n",
    "def crossEntropyLoss(w, b, X, y, reg):\n",
    "    \n",
    "    X, w = augment(X, w, b)\n",
    "    \n",
    "    y_hat = sigmoid(x.dot(W) + b)\n",
    "    L = np.vectorize(lambda x,y: -np.log(x) if y == 1 else -np.log(1-x))(y_hat, y)\n",
    "    #... not finished\n",
    "\n",
    "def gradCE(w, b, X, y, reg):\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(W, b, x, y, alpha, epochs, reg, error_tol, lossType=\"MSE\"):\n",
    "    if lossType == \"MSE\":\n",
    "        return grad_descent_MSE(W, b, x, y, alpha, epochs, reg, error_tol)\n",
    "    elif lossType == \"CE\":\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Variable 'lossType' must be either 'MSE' or 'CE'.\")\n",
    "\n",
    "# Eric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Comparision to Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For zero weight decay, learning rate of 0.005 and 5000 epochs, \n",
    "# plot the training cross entropy loss and MSE loss for \n",
    "# logistic regression and linear regression respectively.\n",
    "# Comment on the effect of cross-entropy loss convergence behaviour.\n",
    "\n",
    "# Sandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Batch Gradient Descent vs. SGD and Adam\n",
    "## 3.1 SGD\n",
    "### 1. Building the Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildGraph(loss=\"MSE\"):\n",
    "    #Initialize weight and bias tensors\n",
    "    tf.set_random_seed(421)\n",
    "\n",
    "    if loss == \"MSE\":\n",
    "        # Your implementation\n",
    "        pass\n",
    "    elif loss == \"CE\":\n",
    "        #Your implementation here\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Variable 'lossType' must be either 'MSE' or 'CE'.\")\n",
    "\n",
    "# Come back to this later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementing Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the SGD algorithm for a minibatch size of 500 \n",
    "# optimizing over 700 epochs 2, minimizing the MSE (you will repeat this for the CE later).\n",
    "# Calculate the total number of batches required by dividing the number\n",
    "# of training instances by the minibatch size. After each epoch you will need to reshuffle the\n",
    "# training data and start sampling from the beginning again. Initially, set \\lambda = 0 and continue\n",
    "# to use the same \\alpha value (i.e. 0.001). After each epoch, store the training, validation and test\n",
    "# losses and accuracies. Use these to plot the loss and accuracy curves.\n",
    "\n",
    "# Dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Batch Size Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hyperparameter Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Cross Entropy Loss Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anyone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Comparison against Batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sandra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
